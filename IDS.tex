\documentclass[12pt,]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmonofont[Mapping=tex-ansi,Scale=0.7]{Source Code Pro}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            pdftitle={Introduction to Data Science},
            pdfauthor={Hui Lin and Ming Li},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter

\title{Introduction to Data Science}
\author{Hui Lin and Ming Li}
\date{2018-11-12}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\chapter*{Preface}\label{preface}


During the first couple years of our career as data scientists, we were
bewildered by all kinds of data science hype. There is a lack of
definition of many basic terminologies such as ``Big Data'' and ``Data
Science.'' How big is big? If someone ran into you asked what data
science was all about, what would you tell them? What is the difference
between the sexy role ``Data Scientist'' and the traditional ``Data
Analyst''? How suddenly came all kinds of machine algorithms? All those
struck us as confusing and vague as real-world data scientists! But we
always felt that there was something real there. After applying data
science for many years, we explored it more and had a much better idea
about data science. And this book is our endeavor to make data science
to a more legitimate field.

\section*{Goal of the Book}\label{goal-of-the-book}


This is an introductory book to data science with a specific focus on
the application. Data Science is a cross-disciplinary subject involving
hands-on experience and business problem-solving exposures. The majority
of existing introduction books on data science are about the modeling
techniques and the implementation of models using R or Python. However,
they fail to introduce data science in a context of the industrial
environment. Moreover, a crucial part, the art of data science in
practice, is often missing. This book intends to fill the gap.

Some key features of this book are as follows:

\begin{itemize}
\item
  It is comprehensive. It covers not only technical skills but also soft
  skills and big data environment in the industry.
\item
  It is hands-on. We provide the data and repeatable R and Python code.
  You can repeat the analysis in the book using the data and code
  provided. We also suggest you perform the analyses with your data
  whenever possible. You can only learn data science by doing it!
\item
  It is based on context. We put methods in the context of industrial
  data science questions.
\item
  Where appropriate, we point you to more advanced materials on models
  to dive deeper
\end{itemize}

\section*{Who This Book Is For}\label{who-this-book-is-for}


Non-mathematical readers will appreciate the emphasis on problem-solving
with real data across a wide variety of applications and the
reproducibility of the companion R and python code.

Readers should know basic statistical ideas, such as correlation and
linear regression analysis. While the text is biased against complex
equations, a mathematical background is needed for advanced topics.

\section*{What This Book Covers}\label{what-this-book-covers}


Based on industry experience, this book outlines the real world scenario
and points out pitfalls data science practitioners should avoid. It also
covers big data cloud platform and the art of data science such as soft
skills. We use R as the main tool and provide code for both R and
Python.

\section*{Conventions}\label{conventions}


\section*{Acknowledgements}\label{acknowledgements}


\chapter*{About the Authors}\label{about-the-authors}


\textbf{Hui Lin} is currently Data Scientist at Netlify where she is
building the data science department. Before Netlify, she was a Data
Scientist at DowDuPont. She was a leader in the company of applying
advanced data science to enhance Marketing and Sales Effectiveness. She
was providing data science leadership for a broad range of predictive
analytics and market research analysis from 2013 to 2018. She is the
co-founder of Central Iowa R User Group, blogger of scientistcafe.com
and 2018 Program Chair of ASA Statistics in Marketing Section. She
enjoys making analytics accessible to a broad audience and teaches
tutorials and workshops for practitioners on data science. She holds MS
and Ph.D.~in statistics from Iowa State University, BS in mathematical
statistics from Beijing Normal University.

\textbf{Ming Li} is currently a Senior Data Scientist at Amazon and an
Adjunct Faculty of Department of Marketing and Business Analytics in
Texas A\&M University - Commerce. He is the Chair of Quality \&
Productivity Section of ASA for 2017. He was a Data Scientist at Walmart
and a Statistical Leader at General Electric Global Research Center. He
obtained his Ph.D.~in Statistics from Iowa State University at 2010.
With deep statistics background and a few years' experience in data
science, he has trained and mentored numerous junior data scientist with
different background such as statistician, programmer, software
developer, database administrator and business analyst. He is also an
Instructor of Amazon's internal Machine Learning University and was one
of the key founding member of Walmart's Analytics Rotational Program
which bridges the skill gaps between new hires and productive data
scientists.(

\mainmatter

\chapter{Introduction}\label{introduction}

Interest in data science is at an all-time high and has exploded in
popularity in the last couple of years. Data scientists today are from
various backgrounds. If someone ran into you ask what data science is
all about, what would you tell them? It is not an easy question to
answer. Data science is one of the areas that everyone is talking about,
but no one can define.

Media has been hyping about ``Data Science'' ``Big Data'' and
``Artificial Intelligence'' over the fast few years. I like this amusing
statement from the internet:

\begin{quote}
``When you're fundraising, it's AI. When you're hiring, it's ML. When
you're implementing, it's logistic regression.''
\end{quote}

For outsiders, data science is whatever magic that can get useful
information out of data. Everyone should have heard about big data. Data
science trainees now need the skills to cope with such big data sets.
What are those skills? You may hear about: Hadoop, a system using
Map/Reduce to process large data sets distributed across a cluster of
computers or about Spark, a system build atop Hadoop for speeding up the
same by loading huge datasets into shared memory(RAM) across clusters.
The new skills are for dealing with organizational artifacts of
large-scale cluster computing but not for better solving the real
problem. A lot of data means more tinkering with computers. After all,
it isn't the size of the data that's important. It's what you do with
it. Your first reaction to all of this might be some combination of
skepticism and confusion. We want to address this up front that: we had
that exact reaction.

To declutter, let's start from a brief history of data science. If you
hit up the Google Trends website which shows search keyword information
over time and check the term ``data science,'' you will find the history
of data science goes back a little further than 2004. From the way media
describes it, you may feel machine learning algorithms were just
invented last month, and there was never ``big'' data before Google.
That is not true. There are new and exciting developments of data
science, but many of the techniques we are using are based on decades of
work by statisticians, computer scientists, mathematicians and
scientists of all types.

In the early 19th century when Legendre and Gauss came up the least
squares method for linear regression, only physicists would use it to
fit linear regression. Now, even non-technical people can fit linear
regressions using excel. In 1936 Fisher came up with linear discriminant
analysis. In the 1940s, we had another widely used model -- logistic
regression. In the 1970s, Nelder and Wedderburn formulated ``generalized
linear model (GLM)'' which:

\begin{quote}
``generalized linear regression by allowing the linear model to be
related to the response variable via a link function and by allowing the
magnitude of the variance of each measurement to be a function of its
predicted value.'' {[}from Wikipedia{]}
\end{quote}

By the end of the 1970s, there was a range of analytical models and most
of them were linear because computers were not powerful enough to fit
non-linear model until the 1980s.

In 1984 Breiman et al\citep{Breiman1984}. introduced the classification
and regression tree (CART) which is one of the oldest and most utilized
classification and regression techniques. After that Ross Quinlan came
up with more tree algorithms such as ID3, C4.5, and C5.0. In the 1990s,
ensemble techniques (methods that combine many models' predictions)
began to appear. Bagging is a general approach that uses bootstrapping
in conjunction with regression or classification model to construct an
ensemble. Based on the ensemble idea, Breiman came up with random forest
in 2001\citep{Breiman2001}. In the same year, Leo Breiman published a
paper ``\href{http://www2.math.uu.se/~thulin/mm/breiman.pdf}{Statistical
Modeling: The Two Cultures}'' \citep{Breiman2001TwoCulture} where he
pointed out two cultures in the use of statistical modeling to get
information from data:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Data is from a given stochastic data model\\
\item
  Data mechanism is unknown and people approach the data using
  algorithmic model
\end{enumerate}

Most of the classic statistical models are the first type. Black box
models, such as random forest, GMB, and today's buzz work deep learning
are algorithmic modeling. As Breiman pointed out, those models can be
used both on large complex data as a more accurate and informative
alternative to data modeling on smaller data sets. Those algorithms have
developed rapidly, however, in fields outside statistics. That is one of
the most important reasons that statisticians are not the mainstream of
today's data science, both in theory and practice. Hence Python is
catching up R as the most commonly used language in data science. It is
due to the data scientists background rather than the language itself.
Since 2000, the approaches to get information out of data have been
shifting from traditional statistical models to a more diverse toolbox
named machine learning.

What is the driving force behind the shifting trend? John Tukey
identified four forces driving data analysis (there was no ``data
science'' back to 1962):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The formal theories of math and statistics
\item
  Acceleration of developments in computers and display devices
\item
  The challenge, in many fields, of more and ever larger bodies of data
\item
  The emphasis on quantification in an ever wider variety of disciplines
\end{enumerate}

Tukey's 1962 list is surprisingly modern. Let's inspect those points in
today's context. People usually develop theories way before they find
the applications. In the past 50 years, statisticians, mathematician,
and computer scientists have been laying the theoretical groundwork for
constructing ``data science'' today. The development of computers
enables us to apply the algorithmic models (they can be very
computationally expensive) and deliver results in a friendly and
intuitive way. The striking transition to the internet of things
generates vast amounts of commercial data. Industries have also sensed
the value of exploiting that data. Data science seems certain to be a
major preoccupation of commercial life in coming decades. All the four
forces John identified exist today and have been driving data science.

Benefiting from the increasing availability of digitized information,
and the possibility to distribute that through the internet, the toolbox
and application have been expanding fast. Today, people apply data
science in a plethora of areas including business, health, biology,
social science, politics, etc. Now data science is everywhere. But what
is today's data science?

\section{Blind men and an elephant}\label{blind-men-and-an-elephant}

There is a widely diffused Chinese parable (depending on where you are
from, you may think it is a Japanese parable) which is about a group of
blind men conceptualizing what the elephant is like by touching it:

\begin{quote}
``\ldots{}In the case of the first person, whose hand landed on the
trunk, said: `This being is like a thick snake'. For another one whose
hand reached its ear, it seemed like a kind of fan. As for another
person, whose hand was upon its leg, said, the elephant is a pillar like
a tree-trunk. The blind man who placed his hand upon its side said,
`elephant is a wall'. Another who felt its tail described it as a rope.
The last felt its tusk, stating the elephant is that which is hard,
smooth and like a spear.''
\href{https://en.wikipedia.org/wiki/Blind_men_and_an_elephant}{wikipedia}
\end{quote}

Data science is the elephant. With the data science hype picking up
stream, many professionals changed their titles to be ``Data Scientist''
without any of the necessary qualifications. Today's data scientists
have vastly different backgrounds, yet each one conceptualizes what the
elephant is based on his/her own professional training and application
area. And to make matters worse, most of us are not even fully aware of
our own conceptualizations, much less the uniqueness of the experience
from which they are derived. Here is a list of somewhat whimsical
definitions for a ``data scientist'':

\begin{itemize}
\tightlist
\item
  ``A data scientist is a data analyst who lives in California''
\item
  ``A data scientist is someone who is better at statistics than any
  software engineer and better at software engineering than any
  statistician.''
\item
  ``A data scientist is a statistician who lives in San Francisco.''
\item
  ``Data Science is statistics on a Mac.''
\end{itemize}

\begin{quote}
``We don't see things as they are, we see them as we are. {[}by Anais
Nin{]}''
\end{quote}

It is annoying but true. So the answer to the question ``what is data
science?'' depends on who you are talking to. Who you may be talking to
then? Data science has three main skill tracks: engineering, analysis,
and modeling (and yes, the order matters!). Here are some representative
skills in each track. Different tracks and combinations of tracks will
define different roles in data science. \footnote{This is based on
  \href{https://github.com/brohrer/academic_advisory}{Industry
  recommendations for academic data science programs:
  https://github.com/brohrer/academic\_advisory} with modifications. It
  is a collection of thoughts of different data scientist across
  industries about what a data scientist does, and what differentiates
  an exceptional data scientist.}

\begin{itemize}
\tightlist
\item
  Engineering: the process of making everything else possible
\end{itemize}

You need to have the ability to get data before making sense of it. If
you only deal with a small dataset, you may be able to get by with
entering some numbers into a spreadsheet. As the data increasing in
size, data engineering becomes a sophisticated discipline in its own
right.

Data engineering mainly involves in building the data pipeline
infrastructure. In the (not that) old day, when data is stored on local
servers, computers or other devices, building the data infrastructure
can be a humongous IT project which involves not only the software but
also the hardware that used to store the data and perform ETL process.
As the development of could service, data storage and computing on the
cloud becomes the new norm. Data engineering today at its core is
software engineering. Ensuring maintainability through modular,
well-commented code and version control is fundamental.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Data management\\
  Automated data collection is a common task which includes parsing the
  logs (depending on the stage of the company and the type of industry
  you are in), web scraping, API queries, and interrogating data
  streams.
\item
  Production If you want to integrate the model or analysis into the
  production system, then you have to automate all data handling steps.
  It involves the whole pipeline from data access to preprocessing,
  modeling and final deployment. It is necessary to make the system work
  smoothly with all existing stacks. So it requires to monitor the
  system through some sort of robust measures, such as rigorous error
  handling, fault tolerance, and graceful degradation to make sure the
  system is running smoothly and the users are happy.
\end{enumerate}

\chapter{Introduction to the data}\label{introduction-to-the-data}

Before tackling analytics problem, we start by introducing data to be
analyzed in later chapters.

\section{Customer Data for Clothing
Company}\label{customer-data-for-clothing-company}

Our first data set represents customers of a clothing company who sells
products in stores and online. This data is typical of what one might
get from a company's marketing data base (the data base will have more
data than the one we show here). This data includes 1000 customers for
whom we have 3 types of data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Demography

  \begin{itemize}
  \tightlist
  \item
    \texttt{age}: age of the respondent
  \item
    \texttt{gender}: male/female
  \item
    \texttt{house}: 0/1 variable indicating if the customer owns a house
    or not
  \end{itemize}
\item
  Sales in the past year

  \begin{itemize}
  \tightlist
  \item
    \texttt{store\_exp}: expense in store
  \item
    \texttt{online\_exp}: expense online
  \item
    \texttt{store\_trans}: times of store purchase
  \item
    \texttt{online\_trans}: times of online purchase
  \end{itemize}
\item
  Survey on product preference
\end{enumerate}

It is common for companies to survey their customers and draw insights
to guide future marketing activities. The survey is as below:

How strongly do you agree or disagree with the following statements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Strong disagree
\item
  Disagree
\item
  Neither agree nor disagree
\item
  Agree
\item
  Strongly agree
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Q1. I like to buy clothes from different brands
\item
  Q2. I buy almost all my clothes from some of my favorite brands
\item
  Q3. I like to buy premium brands
\item
  Q4. Quality is the most important factor in my purchasing decision
\item
  Q5. Style is the most important factor in my purchasing decision
\item
  Q6. I prefer to buy clothes in store
\item
  Q7. I prefer to buy clothes online
\item
  Q8. Price is important
\item
  Q9. I like to try different styles
\item
  Q10. I like to make a choice by myself and don't need too much of
  others' suggestions
\end{itemize}

There are 4 segments of customers:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Price
\item
  Conspicuous
\item
  Quality
\item
  Style
\end{enumerate}

Let's check it:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sim.dat,}\DataTypeTok{vec.len=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    1000 obs. of  19 variables:
##  $ age         : int  57 63 59 60 51 59 57 57 ...
##  $ gender      : Factor w/ 2 levels "Female","Male": 1 1 2 2 2 2 2 2 ...
##  $ income      : num  120963 122008 114202 113616 ...
##  $ house       : Factor w/ 2 levels "No","Yes": 2 2 2 2 2 2 2 2 ...
##  $ store_exp   : num  529 478 491 348 ...
##  $ online_exp  : num  304 110 279 142 ...
##  $ store_trans : int  2 4 7 10 4 4 5 11 ...
##  $ online_trans: int  2 2 2 2 4 5 3 5 ...
##  $ Q1          : int  4 4 5 5 4 4 4 5 ...
##  $ Q2          : int  2 1 2 2 1 2 1 2 ...
##  $ Q3          : int  1 1 1 1 1 1 1 1 ...
##  $ Q4          : int  2 2 2 3 3 2 2 3 ...
##  $ Q5          : int  1 1 1 1 1 1 1 1 ...
##  $ Q6          : int  4 4 4 4 4 4 4 4 ...
##  $ Q7          : int  1 1 1 1 1 1 1 1 ...
##  $ Q8          : int  4 4 4 4 4 4 4 4 ...
##  $ Q9          : int  2 1 1 2 2 1 1 2 ...
##  $ Q10         : int  4 4 4 4 4 4 4 4 ...
##  $ segment     : Factor w/ 4 levels "Conspicuous",..: 2 2 2 2 2 2 2 2 ...
\end{verbatim}

\section{Customer Satisfaction Survey Data from Airline
Company}\label{customer-satisfaction-survey-data-from-airline-company}

This data set is from a customer satisfaction survey for three airline
companies. There are \texttt{N=1000} respondents and 15 questions. The
market researcher asked respondents to recall the experience with
different airline companies and assign a score (1-9) to each airline
company for all the 15 questions. The higher the score, the more
satisfied the customer to the specific item. The 15 questions are of 4
types (the variable names are in the parentheses):

\begin{itemize}
\tightlist
\item
  How satisfied are you with your\_\_\_\_\_\_?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ticketing

  \begin{itemize}
  \tightlist
  \item
    Ease of making reservation（Easy\_Reservation）
  \item
    Availability of preferred seats（Preferred\_Seats）
  \item
    Variety of flight options（Flight\_Options）
  \item
    Ticket prices（Ticket\_Prices）
  \end{itemize}
\item
  Aircraft

  \begin{itemize}
  \tightlist
  \item
    Seat comfort（Seat\_Comfort）
  \item
    Roominess of seat area（Seat\_Roominess）
  \item
    Availability of Overhead（Overhead\_Storage）
  \item
    Cleanliness of aircraft（Clean\_Aircraft）
  \end{itemize}
\item
  Service

  \begin{itemize}
  \tightlist
  \item
    Courtesy of flight attendant（Courtesy）
  \item
    Friendliness（Friendliness）
  \item
    Helpfulness（Helpfulness）
  \item
    Food and drinks（Service）
  \end{itemize}
\item
  General

  \begin{itemize}
  \tightlist
  \item
    Overall satisfaction（Satisfaction）
  \item
    Purchase again（Fly\_Again）
  \item
    Willingness to recommend（Recommend）
  \end{itemize}
\end{enumerate}

Now check the data frame we have:

\begin{verbatim}
## Parsed with column specification:
## cols(
##   Easy_Reservation = col_integer(),
##   Preferred_Seats = col_integer(),
##   Flight_Options = col_integer(),
##   Ticket_Prices = col_integer(),
##   Seat_Comfort = col_integer(),
##   Seat_Roominess = col_integer(),
##   Overhead_Storage = col_integer(),
##   Clean_Aircraft = col_integer(),
##   Courtesy = col_integer(),
##   Friendliness = col_integer(),
##   Helpfulness = col_integer(),
##   Service = col_integer(),
##   Satisfaction = col_integer(),
##   Fly_Again = col_integer(),
##   Recommend = col_integer(),
##   ID = col_integer(),
##   Airline = col_character()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(rating,}\DataTypeTok{vec.len=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'tbl_df', 'tbl' and 'data.frame':    3000 obs. of  17 variables:
##  $ Easy_Reservation: int  6 5 6 5 4 5 6 4 ...
##  $ Preferred_Seats : int  5 7 6 6 5 6 6 6 ...
##  $ Flight_Options  : int  4 7 5 5 3 4 6 3 ...
##  $ Ticket_Prices   : int  5 6 6 5 6 5 5 5 ...
##  $ Seat_Comfort    : int  5 6 7 7 6 6 6 4 ...
##  $ Seat_Roominess  : int  7 8 6 8 7 8 6 5 ...
##  $ Overhead_Storage: int  5 5 7 6 5 4 4 4 ...
##  $ Clean_Aircraft  : int  7 6 7 7 7 7 6 4 ...
##  $ Courtesy        : int  5 6 6 4 2 5 5 4 ...
##  $ Friendliness    : int  4 6 6 6 3 4 5 5 ...
##  $ Helpfulness     : int  6 5 6 4 4 5 5 4 ...
##  $ Service         : int  6 5 6 5 3 5 5 5 ...
##  $ Satisfaction    : int  6 7 7 5 4 6 5 5 ...
##  $ Fly_Again       : int  6 6 6 7 4 5 3 4 ...
##  $ Recommend       : int  3 6 5 5 4 5 6 5 ...
##  $ ID              : int  1 2 3 4 5 6 7 8 ...
##  $ Airline         : chr  "AirlineCo.1" "AirlineCo.1" "AirlineCo.1" ...
##  - attr(*, "spec")=List of 2
##   ..$ cols   :List of 17
##   .. ..$ Easy_Reservation: list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Preferred_Seats : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Flight_Options  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Ticket_Prices   : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Seat_Comfort    : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Seat_Roominess  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Overhead_Storage: list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Clean_Aircraft  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Courtesy        : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Friendliness    : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Helpfulness     : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Service         : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Satisfaction    : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Fly_Again       : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Recommend       : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ ID              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Airline         : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   ..$ default: list()
##   .. ..- attr(*, "class")= chr  "collector_guess" "collector"
##   ..- attr(*, "class")= chr "col_spec"
\end{verbatim}

\chapter{Data Pre-processing}\label{data-pre-processing}

Many data analysis related books focus on models, algorithms and
statistical inferences. However, in practice, raw data is usually not
directly used for modeling. Data preprocessing is the process of
converting raw data into clean data that is proper for modeling. A model
fails for various reasons. One is that the modeler doesn't correctly
preprocess data before modeling. Data preprocessing can significantly
impact model results, such as imputing missing value and handling with
outliers. So data preprocessing is a very critical part.

\begin{figure}
\centering
\includegraphics[width=0.90000\textwidth]{images/DataPre-processing.png}
\caption{Data Pre-processing Outline}
\end{figure}

In real life, depending on the stage of data cleanup, data has the
following types:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Raw data
\item
  Technically correct data
\item
  Data that is proper for the model
\item
  Summarized data
\item
  Data with fixed format
\end{enumerate}

The raw data is the first-hand data that analysts pull from the
database, market survey responds from your clients, the experimental
results collected by the R \& D department, and so on. These data may be
very rough, and R sometimes can't read them directly. The table title
could be multi-line, or the format does not meet the requirements:

\begin{itemize}
\tightlist
\item
  Use 50\% to represent the percentage rather than 0.5, so R will read
  it as a character;
\item
  The missing value of the sales is represented by ``-'' instead of
  space so that R will treat the variable as character or factor type;
\item
  The data is in a slideshow document, or the spreadsheet is not
  ``.csv'' but ``.xlsx''
\item
  \ldots{}
\end{itemize}

Most of the time, you need to clean the data so that R can import them.
Some data format requires a specific package. Technically correct data
is the data, after preliminary cleaning or format conversion, that R (or
another tool you use) can successfully import it.

Assume we have loaded the data into R with reasonable column names,
variable format and so on. That does not mean the data is entirely
correct. There may be some observations that do not make sense, such as
age is negative, the discount percentage is greater than 1, or data is
missing. Depending on the situation, there may be a variety of problems
with the data. It is necessary to clean the data before modeling.
Moreover, different models have different requirements on the data. For
example, some model may require the variables are of consistent scale;
some may be susceptible to outliers or collinearity, some may not be
able to handle categorical variables and so on. The modeler has to
preprocess the data to make it proper for the specific model.

Sometimes we need to aggregate the data. For example, add up the daily
sales to get annual sales of a product at different locations. In
customer segmentation, it is common practice to build a profile for each
segment. It requires calculating some statistics such as average age,
average income, age standard deviation, etc. Data aggregation is also
necessary for presentation, or for data visualization.

The final table results for clients need to be in a nicer format than
what used in the analysis. Usually, data analysts will take the results
from data scientists and adjust the format, such as labels, cell color,
highlight. It is important for a data scientist to make sure the results
look consistent which makes the next step easier for data analysts.

It is highly recommended to store each step of the data and the R code,
making the whole process as repeatable as possible. The R markdown
reproducible report will be extremely helpful for that. If the data
changes, it is easy to rerun the process. In the remainder of this
chapter, we will show the most common data preprocessing methods.

Load the R packages first:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/CE_JSM2017/master/Rcode/00-course-setup.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Data Cleaning}\label{data-cleaning}

After you load the data, the first thing is to check how many variables
are there, the type of variables, the distributions, and data errors.
Let's read and check the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv "}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

Are there any problems? Questionnaire response Q1-Q10 seem reasonable,
the minimum is 1 and maximum is 5. Recall that the questionnaire score
is 1-5. The number of store transactions (store\_trans) and online
transactions (store\_trans) make sense too. Things need to pay attention
are:

\begin{itemize}
\tightlist
\item
  There are some missing values.
\item
  There are outliers for store expenses (\texttt{store\_exp}). The
  maximum value is 50000. Who would spend \$50000 a year buying clothes?
  Is it an imputation error?
\item
  There is a negative value ( -500) in \texttt{store\_exp} which is not
  logical.
\item
  Someone is 300 years old.
\end{itemize}

How to deal with that? Depending on the real situation, if the sample
size is large enough, it will not hurt to delete those problematic
samples. Here we have 1000 observations. Since marketing survey is
usually expensive, it is better to set these values as missing and
impute them instead of deleting the rows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set problematic values as missings}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{age[}\KeywordTok{which}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{age}\OperatorTok{>}\DecValTok{100}\NormalTok{)]<-}\OtherTok{NA}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{store_exp[}\KeywordTok{which}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{store_exp}\OperatorTok{<}\DecValTok{0}\NormalTok{)]<-}\OtherTok{NA}
\CommentTok{# see the results}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{"income"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age            income      
 Min.   :16.00   Min.   : 41776  
 1st Qu.:25.00   1st Qu.: 84930  
 Median :33.00   Median : 93023  
 Mean   :36.64   Mean   :109968  
 3rd Qu.:45.00   3rd Qu.:121535  
 Max.   :69.00   Max.   :317476  
 NA's   :1  
\end{verbatim}

Now we will deal with the missing values in the data.

\section{Missing Values}\label{missing-values}

You can write a whole book about missing value. This section will only
show some of the most commonly used methods without getting too deep
into the topic. Chapter 7 of the book by De Waal, Pannekoek and Scholtus
\citep{Ton2011} makes a concise overview of some of the existing
imputation methods. The choice of specific method depends on the actual
situation. There is no best way.

One question to ask before imputation: Is there any auxiliary
information? Being aware of any auxiliary information is critical. For
example, if the system set customer who did not purchase as missing,
then the real purchasing amount should be 0. Is missing a random
occurrence? If so, it may be reasonable to impute with mean or median.
If not, is there a potential mechanism for the missing data? For
example, older people are more reluctant to disclose their ages in the
questionnaire, so that the absence of age is not completely random. In
this case, the missing values need to be estimated using the
relationship between age and other independent variables. For example,
use variables such as whether they have children, income, and other
survey questions to build a model to predict age.

Also, the purpose of modeling is important for selecting imputation
methods. If the goal is to interpret the parameter estimate or
statistical inference, then it is important to study the missing
mechanism carefully and to estimate the missing values using non-missing
information as much as possible. If the goal is to predict, people
usually will not study the absence mechanism rigorously (but sometimes
the mechanism is obvious). If the absence mechanism is not clear, treat
it as missing at random and use mean, median, or k-nearest neighbor to
impute. Since statistical inference is sensitive to missing values,
researchers from survey statistics have conducted in-depth studies of
various imputation schemes which focus on valid statistical inference.
The problem of missing values in the prediction model is different from
that in the traditional survey. Therefore, there are not many papers on
missing value imputation in the prediction model. Those who want to
study further can refer to Saar-Tsechansky and Provost's comparison of
different imputation methods \citep{missing1} and De Waal, Pannekoek and
Scholtus' book \citep{Ton2011}.

\subsection{Impute missing values with
median/mode}\label{impute-missing-values-with-medianmode}

In the case of missing at random, a common method is to impute with the
mean (continuous variable) or median (categorical variables). You can
use \texttt{impute()} function in \texttt{imputeMissings} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save the result as another object}
\NormalTok{demo_imp<-}\KeywordTok{impute}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"median/mode"}\NormalTok{)}
\CommentTok{# check the first 5 columns, there is no missing values in other columns}
\KeywordTok{summary}\NormalTok{(demo_imp[,}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age           gender        income       house       store_exp      
 Min.   :16.00   Female:446   Min.   : 41776   No :372   Min.   :  155.8  
 1st Qu.:25.00   Male  :326   1st Qu.: 84930   Yes:400   1st Qu.:  202.7  
 Median :33.00                Median : 93023             Median :  293.0  
 Mean   :36.63                Mean   :109968             Mean   : 1265.4  
 3rd Qu.:45.00                3rd Qu.:121535             3rd Qu.:  540.2  
 Max.   :69.00                Max.   :317476             Max.   :50000.0 
\end{verbatim}

After imputation, \texttt{demo\_imp} has no missing value. This method
is straightforward and widely used. The disadvantage is that it does not
take into account the relationship between the variables. When there is
a significant proportion of missing, it will distort the data. In this
case, it is better to consider the relationship between variables and
study the missing mechanism. In the example here, the missing variables
are numeric. If the missing variable is a categorical/factor variable,
the \texttt{impute()} function will impute with the mode.

You can also use \texttt{preProcess()} function, but it is only for
numeric variables, and can not impute categorical variables. Since
missing values here are numeric, we can use the \texttt{preProcess()}
function. The result is the same as the \texttt{impute()} function.
\texttt{PreProcess()} is a powerful function that can link to a variety
of data preprocessing methods. We will use the function later for other
data preprocessing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"medianImpute"}\NormalTok{)}
\NormalTok{demo_imp2<-}\KeywordTok{predict}\NormalTok{(imp,sim.dat)}
\KeywordTok{summary}\NormalTok{(demo_imp2[,}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsection{K-nearest neighbors}\label{k-nearest-neighbors}

K-nearest neighbor (KNN) will find the k closest samples (Euclidian
distance) in the training set and impute the mean of those
``neighbors.''

Use \texttt{preProcess()} to conduct KNN:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"knnImpute"}\NormalTok{,}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\CommentTok{# need to use predict() to get KNN result}
\NormalTok{demo_imp<-}\KeywordTok{predict}\NormalTok{(imp,sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in `[.data.frame`(old, , non_missing_cols, drop = FALSE) : 
  undefined columns selected
\end{verbatim}

Now we get an error saying ``undefined columns selected.'' It is because
\texttt{sim.dat} has non-numeric variables. The \texttt{preProcess()} in
the first line will automatically ignore non-numeric columns, so there
is no error. However, there is a problem when using \texttt{predict()}
to get the result. Removing those variable will solve the problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find factor columns}
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"knnImpute"}\NormalTok{,}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{idx<-}\KeywordTok{which}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(sim.dat,class)}\OperatorTok{==}\StringTok{"factor"}\NormalTok{)}
\NormalTok{demo_imp<-}\KeywordTok{predict}\NormalTok{(imp,sim.dat[,}\OperatorTok{-}\NormalTok{idx])}
\KeywordTok{summary}\NormalTok{(demo_imp[,}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\texttt{lapply(data,class)} can return a list of column class. Here the
data frame is \texttt{sim.dat}, and the following code will give the
list of column class:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# only show the first three elements}
\KeywordTok{lapply}\NormalTok{(sim.dat,class)[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Comparing the KNN result with the previous median imputation, the two
are very different. This is because when you tell the
\texttt{preProcess()} function to use KNN (the option
\texttt{method\ ="\ knnImpute"}), it will automatically standardize the
data. Another way is to use Bagging tree (in the next section). Note
that KNN can not impute samples with the entire row missing. The reason
is straightforward. Since the algorithm uses the average of its
neighbors if none of them has a value, what does it apply to calculate
the mean? Let's append a new row with all values missing to the original
data frame to get a new object called \texttt{temp}. Then apply KNN to
\texttt{temp} and see what happens:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp<-}\KeywordTok{rbind}\NormalTok{(sim.dat,}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(sim.dat)))}
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"knnImpute"}\NormalTok{,}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{idx<-}\KeywordTok{which}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(temp,class)}\OperatorTok{==}\StringTok{"factor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{demo_imp<-}\KeywordTok{predict}\NormalTok{(imp,temp[,}\OperatorTok{-}\NormalTok{idx])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in FUN(newX[, i], ...) : 
  cannot impute when all predictors are missing in the new data point
\end{verbatim}

There is an error saying ``cannot impute when all predictors are missing
in the new data point''. It is easy to fix by finding and removing the
problematic row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idx<-}\KeywordTok{apply}\NormalTok{(temp,}\DecValTok{1}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)) )}
\KeywordTok{as.vector}\NormalTok{(}\KeywordTok{which}\NormalTok{(idx}\OperatorTok{==}\KeywordTok{ncol}\NormalTok{(temp)))}
\end{Highlighting}
\end{Shaded}

It shows that row 1001 is problematic. You can go ahead to delete it.

\subsection{Bagging Tree}\label{bagging-tree}

Bagging (Bootstrap aggregating) was originally proposed by Leo Breiman.
It is one of the earliest ensemble methods \citep{bag1}. When used in
missing value imputation, it will use the remaining variables as
predictors to train a bagging tree and then use the tree to predict the
missing values. Although theoretically, the method is powerful, the
computation is much more intense than KNN. In practice, there is a
trade-off between computation time and the effect. If a median or mean
meet the modeling needs, even bagging tree may improve the accuracy a
little, but the upgrade is so marginal that it does not deserve the
extra time. The bagging tree itself is a model for regression and
classification. Here we use \texttt{preProcess()} to impute
\texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"bagImpute"}\NormalTok{)}
\NormalTok{demo_imp<-}\KeywordTok{predict}\NormalTok{(imp,sim.dat)}
\KeywordTok{summary}\NormalTok{(demo_imp[,}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age           gender        income       house       store_exp      
 Min.   :16.00   Female:554   Min.   : 41776   No :432   Min.   :  155.8  
 1st Qu.:25.00   Male  :446   1st Qu.: 86762   Yes:568   1st Qu.:  205.1  
 Median :36.00                Median : 94739             Median :  329.0  
 Mean   :38.58                Mean   :114665             Mean   : 1357.7  
 3rd Qu.:53.00                3rd Qu.:123726             3rd Qu.:  597.3  
 Max.   :69.00                Max.   :319704             Max.   :50000.0  
\end{verbatim}

\section{Centering and Scaling}\label{centering-and-scaling}

It is the most straightforward data transformation. It centers and
scales a variable to mean 0 and standard deviation 1. It ensures that
the criterion for finding linear combinations of the predictors is based
on how much variation they explain and therefore improves the numerical
stability. Models involving finding linear combinations of the
predictors to explain response/predictors variation need data centering
and scaling, such as PCA \citep{pca1}, PLS \citep{PLS1} and EFA
\citep{EFA1}. You can quickly write code yourself to conduct this
transformation.

Let's standardize the variable \texttt{income} from \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{income<-sim.dat}\OperatorTok{$}\NormalTok{income}
\CommentTok{# calculate the mean of income}
\NormalTok{mux<-}\KeywordTok{mean}\NormalTok{(income,}\DataTypeTok{na.rm=}\NormalTok{T)}
\CommentTok{# calculate the standard deviation of income}
\NormalTok{sdx<-}\KeywordTok{sd}\NormalTok{(income,}\DataTypeTok{na.rm=}\NormalTok{T)}
\CommentTok{# centering}
\NormalTok{tr1<-income}\OperatorTok{-}\NormalTok{mux}
\CommentTok{# scaling}
\NormalTok{tr2<-tr1}\OperatorTok{/}\NormalTok{sdx}
\end{Highlighting}
\end{Shaded}

Or the function \texttt{preProcess()} in package \texttt{caret} can
apply this transformation to a set of predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdat<-}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{"income"}\NormalTok{))}
\CommentTok{# set the "method" option}
\NormalTok{trans<-}\KeywordTok{preProcess}\NormalTok{(sdat,}\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{,}\StringTok{"scale"}\NormalTok{))}
\CommentTok{# use predict() function to get the final result}
\NormalTok{transformed<-}\KeywordTok{predict}\NormalTok{(trans,sdat)}
\end{Highlighting}
\end{Shaded}

Now the two variables are in the same scale:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(transformed)}
\end{Highlighting}
\end{Shaded}

Sometimes you only need to scale the variable. For example, if the model
adds a penalty to the parameter estimates (such as \(L_2\) penalty is
ridge regression and \(L_1\) penalty in LASSO), the variables need to
have a similar scale to ensure a fair variable selection. I am a heavy
user of this kind of penalty-based model in my work, and I used the
following quantile transformation:

\[
x_{ij}^{*}=\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{-j},0.01)}
\]

The reason to use 99\% and 1\% quantile instead of maximum and minimum
values is to resist the impact of outliers.

It is easy to write a function to do it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qscale<-}\ControlFlowTok{function}\NormalTok{(dat)\{}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(dat))\{}
\NormalTok{    up<-}\KeywordTok{quantile}\NormalTok{(dat[,i],}\FloatTok{0.99}\NormalTok{)}
\NormalTok{    low<-}\KeywordTok{quantile}\NormalTok{(dat[,i],}\FloatTok{0.01}\NormalTok{)}
\NormalTok{    diff<-up}\OperatorTok{-}\NormalTok{low}
\NormalTok{    dat[,i]<-(dat[,i]}\OperatorTok{-}\NormalTok{low)}\OperatorTok{/}\NormalTok{diff}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{(dat)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In order to illustrate, let's apply it to some variables from
`demo\_imp2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{demo_imp3<-}\KeywordTok{qscale}\NormalTok{(}\KeywordTok{subset}\NormalTok{(demo_imp2,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"income"}\NormalTok{,}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{)))}
\KeywordTok{summary}\NormalTok{(demo_imp3)}
\end{Highlighting}
\end{Shaded}

After transformation, most of the variables are between 0-1.

\section{Resolve Skewness}\label{resolve-skewness}

\href{https://en.wikipedia.org/wiki/Skewness}{Skewness} is defined to be
the third standardized central moment. The formula for the sample
skewness statistics is:
\[ skewness=\frac{\sum(x_{i}-\bar{x})^{3}}{(n-1)v^{3/2}}\]
\[v=\frac{\sum(x_{i}-\bar{x})^{2}}{(n-1)}\] Skewness=0 means that the
destribution is symmetric, i.e.~the probability of falling on either
side of the distribution's mean is equal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# skew, fig.cap='Shewed Distribution', out.width='80%', fig.asp=.75, # fig.align='center'}
\CommentTok{# need skewness() function from e1071 package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\CommentTok{# random sample 1000 chi-square distribution with df=2}
\CommentTok{# right skew}
\NormalTok{x1<-}\KeywordTok{rchisq}\NormalTok{(}\DecValTok{1000}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DataTypeTok{ncp =} \DecValTok{0}\NormalTok{)}
\CommentTok{# get left skew variable x2 from x1}
\NormalTok{x2<-}\KeywordTok{max}\NormalTok{(x1)}\OperatorTok{-}\NormalTok{x1}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(x2),}\DataTypeTok{main=}\KeywordTok{paste}\NormalTok{(}\StringTok{"left skew, skewnwss ="}\NormalTok{,}\KeywordTok{round}\NormalTok{(}\KeywordTok{skewness}\NormalTok{(x2),}\DecValTok{2}\NormalTok{)), }\DataTypeTok{xlab=}\StringTok{"X2"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(x1),}\DataTypeTok{main=}\KeywordTok{paste}\NormalTok{(}\StringTok{"right skew, skewness ="}\NormalTok{,}\KeywordTok{round}\NormalTok{(}\KeywordTok{skewness}\NormalTok{(x1),}\DecValTok{2}\NormalTok{)), }\DataTypeTok{xlab=}\StringTok{"X1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are different ways may help to remove skewness such as log, square
root or inverse. However, it is often difficult to determine from plots
which transformation is most appropriate for correcting skewness. The
Box-Cox procedure automatically identified a transformation from the
family of power transformations that are indexed by a parameter
\(\lambda\)\citep{BOXCOX1}.

\[
x^{*}=\begin{cases}
\begin{array}{c}
\frac{x^{\lambda}-1}{\lambda}\\
log(x)
\end{array} & \begin{array}{c}
if\ \lambda\neq0\\
if\ \lambda=0
\end{array}\end{cases}
\]

It is easy to see that this family includes log transformation
(\(\lambda=0\)), square transformation (\(\lambda=2\)), square root
(\(\lambda=0.5\)), inverse (\(\lambda=-1\)) and others in-between. We
can still use function \texttt{preProcess()} in package \texttt{caret}
to apply this transformation by chaning the \texttt{method} argument.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{describe}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

It is easy to see the skewed variables. If \texttt{mean} and
\texttt{trimmed} differ a lot, there is very likely outliers. By
default, \texttt{trimmed} reports mean by dropping the top and bottom
10\%. It can be adjusted by setting argument \texttt{trim=}. It is clear
that \texttt{store\_exp} has outliers.

As an example, we will apply Box-Cox transformation on
\texttt{store\_trans} and \texttt{online\_trans}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select the two columns and save them as dat_bc}
\NormalTok{dat_bc<-}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"store_trans"}\NormalTok{,}\StringTok{"online_trans"}\NormalTok{))}
\NormalTok{(trans<-}\KeywordTok{preProcess}\NormalTok{(dat_bc,}\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

The last line of the output shows the estimates of \(\lambda\) for each
variable. As before, use \texttt{predict()} to get the transformed
result:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# bc, fig.cap='Box-Cox Transformation', out.width='80%', fig.asp=.75, # fig.align='center'}
\NormalTok{transformed<-}\KeywordTok{predict}\NormalTok{(trans,dat_bc)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(dat_bc}\OperatorTok{$}\NormalTok{store_trans,}\DataTypeTok{main=}\StringTok{"Before Transformation"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"store_trans"}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(transformed}\OperatorTok{$}\NormalTok{store_trans,}\DataTypeTok{main=}\StringTok{"After Transformation"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"store_trans"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Before the transformation, the \texttt{stroe\_trans} is skewed right.
\texttt{BoxCoxTrans\ ()} can also conduct Box-Cox transform. But note
that \texttt{BoxCoxTrans\ ()} can only be applied to a single variable,
and it is not possible to transform difference columns in a data frame
at the same time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(trans<-}\KeywordTok{BoxCoxTrans}\NormalTok{(dat_bc}\OperatorTok{$}\NormalTok{store_trans))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transformed<-}\KeywordTok{predict}\NormalTok{(trans,dat_bc}\OperatorTok{$}\NormalTok{store_trans)}
\KeywordTok{skewness}\NormalTok{(transformed)}
\end{Highlighting}
\end{Shaded}

The estimate of \(\lambda\) is the same as before (0.1). The skewness of
the original observation is 1.1, and -0.2 after transformation. Although
it is not strictly 0, it is greatly improved.

\section{Resolve Outliers}\label{resolve-outliers}

Even under certain assumptions we can statistically define outliers, it
can be hard to define in some situations. Box plot, histogram and some
other basic visualizations can be used to initially check whether there
are outliers. For example, we can visualize numerical non-survey
variables in \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scm, fig.cap='Use basic visualization to check outliers', out.width='80%', # fig.asp=.75, fig.align='center'}
\CommentTok{# select numerical non-survey data}
\NormalTok{sdat<-}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{"income"}\NormalTok{,}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{,}\StringTok{"store_trans"}\NormalTok{,}\StringTok{"online_trans"}\NormalTok{ ))}
\CommentTok{# use scatterplotMatrix() function from car package}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{scatterplotMatrix}\NormalTok{(sdat,}\DataTypeTok{diagonal=}\StringTok{"boxplot"}\NormalTok{,}\DataTypeTok{smoother=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It is also easy to observe the pair relationship from the plot.
\texttt{age} is negatively correlated with \texttt{online\_trans} but
positively correlated with \texttt{store\_trans}. It seems that older
people tend to purchase from the local store. The amount of expense is
positively correlated with income. Scatterplot matrix like this can
reveal lots of information before modeling.

In addition to visualization, there are some statistical methods to
define outliers, such as the commonly used Z-score. The Z-score for
variable \(\mathbf{Y}\) is defined as:

\[Z_{i}=\frac{Y_{i}-\bar{Y}}{s}\]

where \(\bar{Y}\) and \(s\) are mean and standard deviation for \(Y\).
Z-score is a measurement of the distance between each observation and
the mean. This method may be misleading, especially when the sample size
is small. Iglewicz and Hoaglin proposed to use the modified Z-score to
determine the outlier\citep{mad1}：

\[M_{i}=\frac{0.6745(Y_{i}-\bar{Y})}{MAD}\]

Where MAD is the median of a series of \(|Y_ {i} - \bar{Y}|\), called
the median of the absolute dispersion. Iglewicz and Hoaglin suggest that
the points with the Z-score greater than 3.5 corrected above are
possible outliers. Let's apply it to \texttt{income}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate median of the absolute dispersion for income}
\NormalTok{ymad<-}\KeywordTok{mad}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sdat}\OperatorTok{$}\NormalTok{income))}
\CommentTok{# calculate z-score}
\NormalTok{zs<-(sdat}\OperatorTok{$}\NormalTok{income}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sdat}\OperatorTok{$}\NormalTok{income)))}\OperatorTok{/}\NormalTok{ymad}
\CommentTok{# count the number of outliers}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(zs}\OperatorTok{>}\FloatTok{3.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

According to modified Z-score, variable income has 59 outliers. Refer to
\citep{mad1} for other ways of detecting outliers.

The impact of outliers depends on the model. Some models are sensitive
to outliers, such as linear regression, logistic regression. Some are
pretty robust to outliers, such as tree models, support vector machine.
Also, the outlier is not wrong data. It is real observation so cannot be
deleted at will. If a model is sensitive to outliers, we can use
\emph{spatial sign transformation} \citep{ssp} to minimize the problem.
It projects the original sample points to the surface of a sphere by:

\[x_{ij}^{*}=\frac{x_{ij}}{\sqrt{\sum_{j=1}^{p}x_{ij}^{2}}}\]

where \(x_{ij}\) represents the \(i^{th}\) observation and \(j^{th}\)
variable. As shown in the equation, every observation for sample \(i\)
is divided by its square mode. The denominator is the Euclidean distance
to the center of the p-dimensional predictor space. Three things to pay
attention here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It is important to center and scale the predictor data before using
  this transformation
\item
  Unlike centering or scaling, this manipulation of the predictors
  transforms them as a group
\item
  If there are some variables to remove (for example, highly correlated
  variables), do it before the transformation
\end{enumerate}

Function \texttt{spatialSign()} \texttt{caret} package can conduct the
transformation. Take \texttt{income} and \texttt{age} as an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sst, fig.cap='Spatial sign transformation', out.width='80%', fig.asp=.75, }
\CommentTok{# fig.align='center'\}}
\CommentTok{# KNN imputation}
\NormalTok{sdat<-sim.dat[,}\KeywordTok{c}\NormalTok{(}\StringTok{"income"}\NormalTok{,}\StringTok{"age"}\NormalTok{)]}
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sdat,}\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"knnImpute"}\NormalTok{),}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{sdat<-}\KeywordTok{predict}\NormalTok{(imp,sdat)}
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{spatialSign}\NormalTok{(sdat)}
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(transformed)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(income }\OperatorTok{~}\StringTok{ }\NormalTok{age,}\DataTypeTok{data =}\NormalTok{ sdat,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{main=}\StringTok{"Before"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(income }\OperatorTok{~}\StringTok{ }\NormalTok{age,}\DataTypeTok{data =}\NormalTok{ transformed,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{main=}\StringTok{"After"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Some readers may have found that the above code does not seem to
standardize the data before transformation. Recall the introduction of
KNN, \texttt{preProcess()} with \texttt{method="knnImpute"} by default
will standardize data.

\section{Collinearity}\label{collinearity}

It is probably the technical term known by the most un-technical people.
When two predictors are very strongly correlated, including both in a
model may lead to confusion or problem with a singular matrix. There is
an excellent function in \texttt{corrplot} package with the same name
\texttt{corrplot()} that can visualize correlation structure of a set of
predictors. The function has the option to reorder the variables in a
way that reveals clusters of highly correlated ones.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# corp, fig.cap='Correlation Matrix', out.width='80%', fig.asp=.75, }
\CommentTok{# fig.align='center'\}}
\CommentTok{# select non-survey numerical variables}
\NormalTok{sdat<-}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{"income"}\NormalTok{,}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{,}\StringTok{"store_trans"}\NormalTok{,}\StringTok{"online_trans"}\NormalTok{ ))}
\CommentTok{# use bagging imputation here}
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sdat,}\DataTypeTok{method=}\StringTok{"bagImpute"}\NormalTok{)}
\NormalTok{sdat<-}\KeywordTok{predict}\NormalTok{(imp,sdat)}
\CommentTok{# get the correlation matrix}
\NormalTok{correlation<-}\KeywordTok{cor}\NormalTok{(sdat)}
\CommentTok{# plot }
\KeywordTok{par}\NormalTok{(}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{corrplot.mixed}\NormalTok{(correlation,}\DataTypeTok{order=}\StringTok{"hclust"}\NormalTok{,}\DataTypeTok{tl.pos=}\StringTok{"lt"}\NormalTok{,}\DataTypeTok{upper=}\StringTok{"ellipse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The closer the correlation is to 0, the lighter the color is and the
closer the shape is to a circle. The elliptical means the correlation is
not equal to 0 (because we set the \texttt{upper\ =\ "ellipse"}), the
greater the correlation, the narrower the ellipse. Blue represents a
positive correlation; red represents a negative correlation. The
direction of the ellipse also changes with the correlation. The
correlation coefficient is shown in the lower triangle of the matrix.
The variables relationship from previous scatter matrix are clear here:
the negative correlation between age and online shopping, the positive
correlation between income and amount of purchasing. Some correlation is
very strong ( such as the correlation between \texttt{online\_trans}
and\texttt{age} is -0.85) which means the two variables contain
duplicate information.

Section 3.5 of ``Applied Predictive Modeling'' \citep{APM} presents a
heuristic algorithm to remove a minimum number of predictors to ensure
all pairwise correlations are below a certain threshold:

\begin{quote}
\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Calculate the correlation matrix of the predictors.
\item
  Determine the two predictors associated with the largest absolute
  pairwise correlation (call them predictors A and B).
\item
  Determine the average correlation between A and the other variables.
  Do the same for predictor B.
\item
  If A has a larger average correlation, remove it; otherwise, remove
  predictor B.
\item
  Repeat Step 2-4 until no absolute correlations are above the
  threshold.
\end{enumerate}
\end{quote}

The \texttt{findCorrelation()} function in package \texttt{caret} will
apply the above algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(highCorr<-}\KeywordTok{findCorrelation}\NormalTok{(}\KeywordTok{cor}\NormalTok{(sdat),}\DataTypeTok{cutoff=}\NormalTok{.}\DecValTok{75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

It returns the index of columns need to be deleted. It tells us that we
need to remove the first column to make sure the correlations are all
below 0.75.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# delete highly correlated columns}
\NormalTok{sdat<-sdat[}\OperatorTok{-}\NormalTok{highCorr]}
\CommentTok{# check the new correlation matrix}
\KeywordTok{cor}\NormalTok{(sdat)}
\end{Highlighting}
\end{Shaded}

The absolute value of the elements in the correlation matrix after
removal are all below 0.75. How strong does a correlation have to get,
before you should start worrying about multicollinearity? There is no
easy answer to that question. You can treat the threshold as a tuning
parameter and pick one that gives you best prediction accuracy.

\section{Sparse Variables}\label{sparse-variables}

Other than the highly related predictors, predictors with degenerate
distributions can cause the problem too. Removing those variables can
significantly improve some models' performance and stability (such as
linear regression and logistic regression but the tree based model is
impervious to this type of predictors). One extreme example is a
variable with a single value which is called zero-variance variable.
Variables with very low frequency of unique values are near-zero
variance predictors. In general, detecting those variables follows two
rules:

\begin{itemize}
\tightlist
\item
  The fraction of unique values over the sample size
\item
  The ratio of the frequency of the most prevalent value to the
  frequency of the second most prevalent value.
\end{itemize}

\texttt{nearZeroVar()} function in the \texttt{caret} package can filter
near-zero variance predictors according to the above rules. In order to
show the useage of the function, let's arbitaryly add some problematic
variables to the origional data \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make a copy}
\NormalTok{zero_demo<-sim.dat}
\CommentTok{# add two sparse variable}
\CommentTok{# zero1 only has one unique value}
\CommentTok{# zero2 is a vector with the first element 1 and the rest are 0s}
\NormalTok{zero_demo}\OperatorTok{$}\NormalTok{zero1<-}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(zero_demo))}
\NormalTok{zero_demo}\OperatorTok{$}\NormalTok{zero2<-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(zero_demo)}\OperatorTok{-}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The function will return a vector of integers indicating which columns
to remove:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nearZeroVar}\NormalTok{(zero_demo,}\DataTypeTok{freqCut =} \DecValTok{95}\OperatorTok{/}\DecValTok{5}\NormalTok{, }\DataTypeTok{uniqueCut =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As expected, it returns the two columns we generated. You can go ahead
to remove them. Note the two arguments in the function
\texttt{freqCut\ =} and \texttt{uniqueCut\ =} are corresponding to the
previous two rules.

\begin{itemize}
\tightlist
\item
  \texttt{freqCut}: the cutoff for the ratio of the most common value to
  the second most common value
\item
  \texttt{uniqueCut}: the cutoff for the percentage of distinct values
  out of the number of total samples
\end{itemize}

\section{Re-encode Dummy Variables}\label{re-encode-dummy-variables}

A dummy variable is a binary variable (0/1) to represent subgroups of
the sample. Sometimes we need to recode categories to smaller bits of
information named ``dummy variables.'' For example, some questionnaires
have five options for each question, A, B, C, D, and E. After you get
the data, you will usually convert the corresponding categorical
variables for each question into five nominal variables, and then use
one of the options as the baseline.

Let's encode \texttt{gender} and \texttt{house} from \texttt{sim.dat} to
dummy variables. There are two ways to implement this. The first is to
use \texttt{class.ind()} from \texttt{nnet} package. However, it only
works on one variable at a time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumVar<-nnet}\OperatorTok{::}\KeywordTok{class.ind}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{gender)}
\KeywordTok{head}\NormalTok{(dumVar)}
\end{Highlighting}
\end{Shaded}

Since it is redundant to keep both, we need to remove one of them when
modeling. Another more powerful function is \texttt{dummyVars()} from
\texttt{caret}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumMod<-}\KeywordTok{dummyVars}\NormalTok{(}\OperatorTok{~}\NormalTok{gender}\OperatorTok{+}\NormalTok{house}\OperatorTok{+}\NormalTok{income,}
                  \DataTypeTok{data=}\NormalTok{sim.dat,}
                  \CommentTok{# use "origional variable name + level" as new name}
                  \DataTypeTok{levelsOnly=}\NormalTok{F)}
\KeywordTok{head}\NormalTok{(}\KeywordTok{predict}\NormalTok{(dumMod,sim.dat))}
\end{Highlighting}
\end{Shaded}

\texttt{dummyVars()} can also use formula format. The variable on the
right-hand side can be both categorical and numeric. For a numerical
variable, the function will keep the variable unchanged. The advantage
is that you can apply the function to a data frame without removing
numerical variables. Other than that, the function can create
interaction term:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumMod<-}\KeywordTok{dummyVars}\NormalTok{(}\OperatorTok{~}\NormalTok{gender}\OperatorTok{+}\NormalTok{house}\OperatorTok{+}\NormalTok{income}\OperatorTok{+}\NormalTok{income}\OperatorTok{:}\NormalTok{gender,}
                  \DataTypeTok{data=}\NormalTok{sim.dat,}
                  \DataTypeTok{levelsOnly=}\NormalTok{F)}
\KeywordTok{head}\NormalTok{(}\KeywordTok{predict}\NormalTok{(dumMod,sim.dat))}
\end{Highlighting}
\end{Shaded}

If you think the impact income levels on purchasing behavior is
different for male and female, then you may add the interaction term
between \texttt{income} and \texttt{gender}. You can do this by adding
\texttt{income:\ gender} in the formula.

\section{Python Computing}\label{python-computing}

\textbf{Environmental Setup}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ IPython.core.interactiveshell }\ImportTok{import}\NormalTok{ InteractiveShell}
\NormalTok{InteractiveShell.ast_node_interactivity }\OperatorTok{=} \StringTok{"all"}

\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ scipy }\ImportTok{as}\NormalTok{ sp}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ math}

\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ Imputer}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}

\ImportTok{from}\NormalTok{ pandas.plotting }\ImportTok{import}\NormalTok{ scatter_matrix}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{Data Cleaning}\label{data-cleaning-1}

\chapter{Data Wrangling}\label{data-wrangling}

This chapter focuses on some of the most frequently used data
manipulations and shows how to implement them in R and Python. It is
critical to explore the data with descriptive statistics (mean, standard
deviation, etc.) and data visualization before analysis. Transform data
so that the data structure is in line with the requirements of the
model. You also need to summarize the results after analysis.

\section{Data Wrangling Using R}\label{data-wrangling-using-r}

\subsection{Read and write data}\label{read-and-write-data}

\subsubsection{\texorpdfstring{\texttt{readr}}{readr}}\label{readr}

You must be familiar with \texttt{read.csv()}, \texttt{read.table()} and
\texttt{write.csv()} in base R. Here we will introduce a more efficient
package from RStudio in 2015 for reading and writing data:
\texttt{readr} package. The corresponding functions are
\texttt{read\_csv()}, \texttt{read\_table()} and \texttt{write\_csv()}.
The commands look quite similar, but \texttt{readr} is different in the
following respects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It is 10x faster. The trick is that \texttt{readr} uses C++ to process
  the data quickly.
\item
  It doesn't change the column names. The names can start with a number
  and ``\texttt{.}'' will not be substituted to ``\texttt{\_}''. For
  example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"2015,2016,2017}
\StringTok{1,2,3}
\StringTok{4,5,6"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{readr} functions do not convert strings to factors by default,
  are able to parse dates and times and can automatically determine the
  data types in each column.
\item
  The killing character, in my opinion, is that \texttt{readr} provides
  \textbf{progress bar}. What makes you feel worse than waiting is not
  knowing how long you have to wait.
\end{enumerate}

\includegraphics{images/prograssbar.png}~

The major functions of readr is to turn flat files into data frames:

\begin{itemize}
\tightlist
\item
  \texttt{read\_csv()}: reads comma delimited files
\item
  \texttt{read\_csv2()}: reads semicolon separated files (common in
  countries where \texttt{,} is used as the decimal place)
\item
  \texttt{read\_tsv()}: reads tab delimited files
\item
  \texttt{read\_delim()}: reads in files with any delimiter
\item
  \texttt{read\_fwf()}: reads fixed width files. You can specify fields
  either by their widths with \texttt{fwf\_widths()} or their position
  with \texttt{fwf\_positions()}\\
\item
  \texttt{read\_table()}: reads a common variation of fixed width files
  where columns are separated by white space
\item
  \texttt{read\_log()}: reads Apache style log files
\end{itemize}

The good thing is that those functions have similar syntax. Once you
learn one, the others become easy. Here we will focus on
\texttt{read\_csv()}.

The most important information for \texttt{read\_csv()} is the path to
your data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv "}\NormalTok{)}
\KeywordTok{head}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

The function reads the file to R as a \texttt{tibble}. You can consider
\texttt{tibble} as next iteration of the data frame. They are different
with data frame for the following aspects:

\begin{itemize}
\tightlist
\item
  It never changes an input's type (i.e., no more
  \texttt{stringsAsFactors\ =\ FALSE}!)
\item
  It never adjusts the names of variables
\item
  It has a refined print method that shows only the first 10 rows and
  all the columns that fit on the screen. You can also control the
  default print behavior by setting options.
\end{itemize}

Refer to \url{http://r4ds.had.co.nz/tibbles.html} for more information
about `tibble'.

When you run \texttt{read\_csv()} it prints out a column specification
that gives the name and type of each column. To better understanding how
\texttt{readr} works, it is helpful to type in some baby data set and
check the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"2015,2016,2017}
\StringTok{100,200,300}
\StringTok{canola,soybean,corn"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

You can also add comments on the top and tell R to skip those lines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"# I will never let you know that}
\StringTok{          # my favorite food is carrot}
\StringTok{          Date,Food,Mood}
\StringTok{          Monday,carrot,happy}
\StringTok{          Tuesday,carrot,happy}
\StringTok{          Wednesday,carrot,happy}
\StringTok{          Thursday,carrot,happy}
\StringTok{          Friday,carrot,happy}
\StringTok{          Saturday,carrot,extremely happy}
\StringTok{          Sunday,carrot,extremely happy"}\NormalTok{, }\DataTypeTok{skip =} \DecValTok{2}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

If you don't have column names, set \texttt{col\_names\ =\ FALSE} then R
will assign names ``\texttt{X1}'',``\texttt{X2}''\ldots{} to the
columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Saturday,carrot,extremely happy}
\StringTok{          Sunday,carrot,extremely happy"}\NormalTok{, }\DataTypeTok{col_names=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

You can also pass \texttt{col\_names} a character vector which will be
used as the column names. Try to replace \texttt{col\_names=FALSE} with
\texttt{col\_names=c("Date","Food","Mood")} and see what happen.

As mentioned before, you can use \texttt{read\_csv2()} to read semicolon
separated files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv2}\NormalTok{(}\StringTok{"Saturday; carrot; extremely happy }\CharTok{\textbackslash{}n}\StringTok{ Sunday; carrot; extremely happy"}\NormalTok{, }\DataTypeTok{col_names=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

Here ``\texttt{\textbackslash{}n}'' is a convenient shortcut for adding
a new line.

You can use \texttt{read\_tsv()} to read tab delimited files：

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_tsv}\NormalTok{(}\StringTok{"every}\CharTok{\textbackslash{}t}\StringTok{man}\CharTok{\textbackslash{}t}\StringTok{is}\CharTok{\textbackslash{}t}\StringTok{a}\CharTok{\textbackslash{}t}\StringTok{poet}\CharTok{\textbackslash{}t}\StringTok{when}\CharTok{\textbackslash{}t}\StringTok{he}\CharTok{\textbackslash{}t}\StringTok{is}\CharTok{\textbackslash{}t}\StringTok{in}\CharTok{\textbackslash{}t}\StringTok{love}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

Or more generally, you can use \texttt{read\_delim()} and assign
separating character：

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_delim}\NormalTok{(}\StringTok{"THE|UNBEARABLE|RANDOMNESS|OF|LIFE}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{delim =} \StringTok{"|"}\NormalTok{, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

Another situation you will often run into is the missing value. In
marketing survey, people like to use ``99'' to represent missing. You
can tell R to set all observation with value ``99'' as missing when you
read the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Q1,Q2,Q3}
\StringTok{               5, 4,99"}\NormalTok{,}\DataTypeTok{na=}\StringTok{"99"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

For writing data back to disk, you can use \texttt{write\_csv()} and
\texttt{write\_tsv()}. The following two characters of the two functions
increase the chances of the output file being read back in correctly:

\begin{itemize}
\tightlist
\item
  Encode strings in UTF-8
\item
  Save dates and date-times in ISO8601 format so they are easily parsed
  elsewhere
\end{itemize}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write_csv}\NormalTok{(sim.dat, }\StringTok{"sim_dat.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For other data types, you can use the following packages:

\begin{itemize}
\tightlist
\item
  \texttt{Haven}: SPSS, Stata and SAS data
\item
  \texttt{Readxl} and \texttt{xlsx}: excel data(.xls and .xlsx)
\item
  \texttt{DBI}: given data base, such as RMySQL, RSQLite and
  RPostgreSQL, read data directly from the database using SQL
\end{itemize}

Some other useful materials:

\begin{itemize}
\tightlist
\item
  For getting data from the internet, you can refer to the book ``XML
  and Web Technologies for Data Sciences with R''.\\
\item
  \href{https://cran.r-project.org/doc/manuals/r-release/R-data.html\#Acknowledgements}{R
  data import/export manual}
\item
  \texttt{rio} package：\url{https://github.com/leeper/rio}
\end{itemize}

\subsubsection{\texorpdfstring{\texttt{data.table}--- enhanced
\texttt{data.frame}}{data.table--- enhanced data.frame}}\label{data.table-enhanced-data.frame}

What is \texttt{data.table}? It is an R package that provides an
enhanced version of \texttt{data.frame}. The most used object in R is
\texttt{data\ frame}. Before we move on, let's briefly review some basic
characters and manipulations of data.frame:

\begin{itemize}
\tightlist
\item
  It is a set of rows and columns.
\item
  Each row is of the same length and data type
\item
  Every column is of the same length but can be of differing data types
\item
  It has characteristics of both a matrix and a list
\item
  It uses \texttt{{[}{]}} to subset data
\end{itemize}

We will use the clothes customer data to illustrate. There are two
dimensions in \texttt{{[}{]}}. The first one indicates the row and
second one indicates column. It uses a comma to separate them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data}
\NormalTok{sim.dat<-readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv"}\NormalTok{)}
\CommentTok{# subset the first two rows}
\NormalTok{sim.dat[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,]}
\CommentTok{# subset the first two rows and column 3 and 5}
\NormalTok{sim.dat[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{)]}
\CommentTok{# get all rows with age>70}
\NormalTok{sim.dat[sim.dat}\OperatorTok{$}\NormalTok{age}\OperatorTok{>}\DecValTok{70}\NormalTok{,]}
\CommentTok{# get rows with age> 60 and gender is Male}
\CommentTok{# select column 3 and 4}
\NormalTok{sim.dat[sim.dat}\OperatorTok{$}\NormalTok{age}\OperatorTok{>}\DecValTok{68} \OperatorTok{&}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{gender }\OperatorTok{==}\StringTok{ "Male"}\NormalTok{, }\DecValTok{3}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Remember that there are usually different ways to conduct the same
manipulation. For example, the following code presents three ways to
calculate an average number of online transactions for male and female:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tapply}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{online_trans, sim.dat}\OperatorTok{$}\NormalTok{gender, mean )}

\KeywordTok{aggregate}\NormalTok{(online_trans }\OperatorTok{~}\StringTok{ }\NormalTok{gender, }\DataTypeTok{data =}\NormalTok{ sim.dat, mean)}

\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{sim.dat}\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Avg_online_trans=}\KeywordTok{mean}\NormalTok{(online_trans))}
\end{Highlighting}
\end{Shaded}

There is no gold standard to choose a specific function to manipulate
data. The goal is to solve the real problem, not the tool itself. So
just use whatever tool that is convenient for you.

The way to use \texttt{{[}{]}} is straightforward. But the manipulations
are limited. If you need more complicated data reshaping or aggregation,
there are other packages to use such as \texttt{dplyr},
\texttt{reshape2}, \texttt{tidyr} etc. But the usage of those packages
are not as straightforward as \texttt{{[}{]}}. You often need to change
functions. Keeping related operations together, such as subset, group,
update, join etc, will allow for:

\begin{itemize}
\tightlist
\item
  concise, consistent and readable syntax irrespective of the set of
  operations you would like to perform to achieve your end goal
\item
  performing data manipulation fluidly without the cognitive burden of
  having to change among different functions
\item
  by knowing precisely the data required for each operation, you can
  automatically optimize operations effectively
\end{itemize}

\texttt{data.table} is the package for that. If you are not familiar
with other data manipulating packages and are interested in reducing
programming time tremendously, then this package is for you.

Other than extending the function of \texttt{{[}{]}},
\texttt{data.table} has the following advantages:

Offers fast import, subset, grouping, update, and joins for large data
files It is easy to turn data frame to data table Can behave just like a
data frame

You need to install and load the package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# If you haven't install it, use the code to instal}
\CommentTok{# install.packages("data.table")}
\CommentTok{# load packagw}
\KeywordTok{library}\NormalTok{(data.table)}
\end{Highlighting}
\end{Shaded}

Use \texttt{data.table()} to covert the existing data frame
\texttt{sim.dat} to data table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(sim.dat)}
\KeywordTok{class}\NormalTok{(dt)}
\end{Highlighting}
\end{Shaded}

Calculate mean for counts of online transactions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[, }\KeywordTok{mean}\NormalTok{(online_trans)]}
\end{Highlighting}
\end{Shaded}

You can't do the same thing using data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat[,}\KeywordTok{mean}\NormalTok{(online_trans)]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Error in mean(online_trans) : object 'online_trans' not found}
\end{Highlighting}
\end{Shaded}

If you want to calculate mean by group as before, set ``\texttt{by\ =}''
argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{gender]}
\end{Highlighting}
\end{Shaded}

You can group by more than one variables. For example, group by
``\texttt{gender}'' and ``\texttt{house}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

Assign column names for aggregated variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

\texttt{data.table} can accomplish all operations that
\texttt{aggregate()} and \texttt{tapply()}can do for data frame.

\begin{itemize}
\tightlist
\item
  General setting of \texttt{data.table}
\end{itemize}

Different from data frame, there are three arguments for data table:

\includegraphics{images/datable1.png}

It is analogous to SQL. You don't have to know SQL to learn data table.
But experience with SQL will help you understand data table. In SQL, you
select column \texttt{j} (use command \texttt{SELECT}) for row
\texttt{i} (using command \texttt{WHERE}). \texttt{GROUP\ BY} in SQL
will assign the variable to group the observations.

\includegraphics{images/rSQL.png}

Let's review our previous code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{gender]}
\end{Highlighting}
\end{Shaded}

The code above is equal to the following SQL：

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{  gender, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{FROM}\NormalTok{ sim.dat }\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ gender}
\end{Highlighting}
\end{Shaded}

R code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

is equal to SQL：

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ gender, house, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{AS} \FunctionTok{avg} \KeywordTok{FROM}\NormalTok{ sim.dat }\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ gender, house}
\end{Highlighting}
\end{Shaded}

R code：

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ age }\OperatorTok{<}\StringTok{ }\DecValTok{40}\NormalTok{, .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

is equal to SQL：

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ gender, house, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{AS} \FunctionTok{avg} \KeywordTok{FROM}\NormalTok{ sim.dat }\KeywordTok{WHERE}\NormalTok{ age < }\DecValTok{40} \KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ gender, house}
\end{Highlighting}
\end{Shaded}

You can see the analogy between \texttt{data.table} and \texttt{SQL}.
Now let's focus on operations in data table.

\begin{itemize}
\tightlist
\item
  select row
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select rows with age<20 and income > 80000}
\NormalTok{dt[age }\OperatorTok{<}\StringTok{ }\DecValTok{20} \OperatorTok{&}\StringTok{ }\NormalTok{income }\OperatorTok{>}\StringTok{ }\DecValTok{80000}\NormalTok{]}
\CommentTok{# select the first two rows}
\NormalTok{dt[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  select column
\end{itemize}

Selecting columns in \texttt{data.table} don't need \texttt{\$}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select column “age” but return it as a vector}
\CommentTok{# the argument for row is empty so the result will return all observations}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, age]}
\KeywordTok{head}\NormalTok{(ans)}
\end{Highlighting}
\end{Shaded}

To return \texttt{data.table} object, put column names in
\texttt{list()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Select age and online_exp columns and return as a data.table instead}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, }\KeywordTok{list}\NormalTok{(age, online_exp)]}
\KeywordTok{head}\NormalTok{(ans)}
\end{Highlighting}
\end{Shaded}

Or you can also put column names in \texttt{.()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, .(age, online_exp)]}
\CommentTok{# head(ans)}
\end{Highlighting}
\end{Shaded}

To select all columns from ``\texttt{age}'' to ``\texttt{income}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, age}\OperatorTok{:}\NormalTok{income, with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\KeywordTok{head}\NormalTok{(ans,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Delete columns using \texttt{-} or \texttt{!}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# delete columns from  age to online_exp}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, }\OperatorTok{-}\NormalTok{(age}\OperatorTok{:}\NormalTok{online_exp), with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, }\OperatorTok{!}\NormalTok{(age}\OperatorTok{:}\NormalTok{online_exp), with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  tabulation
\end{itemize}

In data table. \texttt{.N} means to count。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# row count}
\NormalTok{dt[, .N] }
\end{Highlighting}
\end{Shaded}

If you assign the group variable, then it will count by groups:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# counts by gender}
\NormalTok{dt[, .N, by=}\StringTok{ }\NormalTok{gender]  }
\CommentTok{# for those younger than 30, count by gender}
\NormalTok{ dt[age }\OperatorTok{<}\StringTok{ }\DecValTok{30}\NormalTok{, .(}\DataTypeTok{count=}\NormalTok{.N), by=}\StringTok{ }\NormalTok{gender] }
\end{Highlighting}
\end{Shaded}

Order table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get records with the highest 5 online expense:}
\KeywordTok{head}\NormalTok{(dt[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{online_exp)],}\DecValTok{5}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Since data table keep some characters of data frame, they share some
operations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{online_exp)][}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

You can also order the table by more than one variable. The following
code will order the table by \texttt{gender}, then order within
\texttt{gender} by \texttt{online\_exp}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[}\KeywordTok{order}\NormalTok{(gender, }\OperatorTok{-}\NormalTok{online_exp)][}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Use \texttt{fread()} to import dat
\end{itemize}

Other than \texttt{read.csv} in base R, we have introduced `read\_csv'
in `readr'. \texttt{read\_csv} is much faster and will provide progress
bar which makes user feel much better (at least make me feel better).
\texttt{fread()} in \texttt{data.table} further increase the efficiency
of reading data. The following are three examples of reading the same
data file \texttt{topic.csv}. The file includes text data scraped from
an agriculture forum with 209670 rows and 6 columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  user  system elapsed }
\NormalTok{  4.313   0.027   4.340}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   user  system elapsed }
\NormalTok{  0.267   0.008   0.274 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-data.table}\OperatorTok{::}\KeywordTok{fread}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   user  system elapsed }
\NormalTok{  0.217   0.005   0.221 }
\end{Highlighting}
\end{Shaded}

It is clear that \texttt{read\_csv()} is much faster than
\texttt{read.csv()}. \texttt{fread()} is a little faster than
\texttt{read\_csv()}. As the size increasing, the difference will become
for significant. Note that \texttt{fread()} will read file as
\texttt{data.table} by default.

\subsection{Summarize data}\label{summarize-data}

\subsubsection{\texorpdfstring{\texttt{apply()}, \texttt{lapply()} and
\texttt{sapply()} in base
R}{apply(), lapply() and sapply() in base R}}\label{apply-lapply-and-sapply-in-base-r}

There are some powerful functions to summarize data in base R, such as
\texttt{apply()}, \texttt{lapply()} and \texttt{sapply()}. They do the
same basic things and are all from ``apply'' family: apply functions
over parts of data. They differ in two important respects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the type of object they apply to
\item
  the type of result they will return
\end{enumerate}

When do we use \texttt{apply()}? When we want to apply a function to
margins of an array or matrix. That means our data need to be
structured. The operations can be very flexible. It returns a vector or
array or list of values obtained by applying a function to margins of an
array or matrix.

For example you can compute row and column sums for a matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## simulate a matrix}
\NormalTok{x <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{x1 =}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{, }\DataTypeTok{x2 =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\DecValTok{2}\OperatorTok{:}\DecValTok{5}\NormalTok{))}
\KeywordTok{dimnames}\NormalTok{(x)[[}\DecValTok{1}\NormalTok{]] <-}\StringTok{ }\NormalTok{letters[}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{]}
\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{col.sums <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, sum)}
\NormalTok{row.sums <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(x, }\DecValTok{1}\NormalTok{, sum)}
\end{Highlighting}
\end{Shaded}

You can also apply other functions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ma <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{6}\OperatorTok{:}\DecValTok{8}\NormalTok{), }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{ma}
\KeywordTok{apply}\NormalTok{(ma, }\DecValTok{1}\NormalTok{, table)  }\CommentTok{#--> a list of length 2}
\KeywordTok{apply}\NormalTok{(ma, }\DecValTok{1}\NormalTok{, stats}\OperatorTok{::}\NormalTok{quantile) }\CommentTok{# 5 x n matrix with rownames}
\end{Highlighting}
\end{Shaded}

Results can have different lengths for each call. This is a trickier
example. What will you get?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Example with different lengths for each call}
\NormalTok{z <-}\StringTok{ }\KeywordTok{array}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{24}\NormalTok{, }\DataTypeTok{dim =} \DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{)}
\NormalTok{zseq <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(z, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{max}\NormalTok{(x)))}
\NormalTok{zseq         ## a 2 x 3 matrix}
\KeywordTok{typeof}\NormalTok{(zseq) ## list}
\KeywordTok{dim}\NormalTok{(zseq) ## 2 3}
\NormalTok{zseq[}\DecValTok{1}\NormalTok{,]}
\KeywordTok{apply}\NormalTok{(z, }\DecValTok{3}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{max}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{lapply()} applies a function over a list, data.frame or vector
  and returns a list of the same length.
\item
  \texttt{sapply()} is a user-friendly version and wrapper of
  \texttt{lapply()}. By default it returns a vector, matrix or if
  \texttt{simplify\ =\ "array"}, an array if appropriate.
  \texttt{apply(x,\ f,\ simplify\ =\ FALSE,\ USE.NAMES\ =\ FALSE)} is
  the same as \texttt{lapply(x,\ f)}. If \texttt{simplify=TRUE}, then it
  will return a \texttt{data.frame} instead of \texttt{list}.
\end{itemize}

Let's use some data with context to help you better understand the
functions.

\begin{itemize}
\tightlist
\item
  Get the mean and standard deviation of all numerical variables in the
  dataset.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read data}
\NormalTok{sim.dat<-}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv"}\NormalTok{)}
\CommentTok{# Get numerical variables}
\NormalTok{sdat<-sim.dat[,}\OperatorTok{!}\KeywordTok{lapply}\NormalTok{(sim.dat,class)}\OperatorTok{==}\StringTok{"factor"}\NormalTok{]}
\NormalTok{## Try the following code with apply() function}
\NormalTok{## apply(sim.dat,2,class)}
\NormalTok{## What is the problem?}
\end{Highlighting}
\end{Shaded}

The data frame \texttt{sdat} only includes numeric columns. Now we can
go head and use \texttt{apply()} to get mean and standard deviation for
each column:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(sdat, }\DataTypeTok{MARGIN=}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

Here we defined a function using \texttt{function(x)\ mean(na.omit(x))}.
It is a very simple function. It tells R to ignore the missing value
when calculating the mean. \texttt{MARGIN=2} tells R to apply the
function to each column. It is not hard to guess what \texttt{MARGIN=1}
mean. The result show that the average online expense is much higher
than store expense. You can also compare the average scores across
different questions. The command to calculate standard deviation is very
similar. The only difference is to change \texttt{mean()} to
\texttt{sd()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(sdat, }\DataTypeTok{MARGIN=}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

Even the average online expense is higher than store expense, the
standard deviation for store expense is much higher than online expense
which indicates there is very likely some big/small purchase in store.
We can check it quickly:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sdat}\OperatorTok{$}\NormalTok{store_exp)}
\KeywordTok{summary}\NormalTok{(sdat}\OperatorTok{$}\NormalTok{online_exp)}
\end{Highlighting}
\end{Shaded}

There are some odd values in store expense. The minimum value is -500
which is a wrong imputation which indicates that you should preprocess
data before analyzing it. Checking those simple statistics will help you
better understand your data. It then gives you some idea how to
preprocess and analyze them. How about using \texttt{lapply()} and
\texttt{sapply()}?

Run the following code and compare the results:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lapply}\NormalTok{(sdat, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\KeywordTok{sapply}\NormalTok{(sdat, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\KeywordTok{sapply}\NormalTok{(sdat, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)), }\DataTypeTok{simplify =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{dplyr}
package}{dplyr package}}\label{dplyr-package}

\texttt{dplyr} provides a flexible grammar of data manipulation focusing
on tools for working with data frames (hence the \texttt{d} in the
name). It is faster and more friendly:

\begin{itemize}
\tightlist
\item
  It identifies the most important data manipulations and make they easy
  to use from R
\item
  It performs faster for in-memory data by writing key pieces in C++
  using \texttt{Rcpp}
\item
  The interface is the same for data frame, data table or database.
\end{itemize}

We will illustrate the following functions in order:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Display
\item
  Subset
\item
  Summarize
\item
  Create new variable
\item
  Merge
\end{enumerate}

\textbf{Display}

\begin{itemize}
\tightlist
\item
  \texttt{tbl\_df()}: Convert the data to \texttt{tibble} which offers
  better checking and printing capabilities than traditional data
  frames. It will adjust output width according to fit the current
  window.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{tbl_df}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{glimpse()}: This is like a transposed version of
  \texttt{tbl\_df()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\textbf{Subset}

Get rows with \texttt{income} more than 300000:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(magrittr)}
\KeywordTok{filter}\NormalTok{(sim.dat, income }\OperatorTok{>}\DecValTok{300000}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Here we meet a new operator \texttt{\%\textgreater{}\%}. It is called
``Pipe operator'' which pipes a value forward into an expression or
function call. What you get in the left operation will be the first
argument or the only argument in the right operation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{f}\NormalTok{(y) =}\StringTok{ }\KeywordTok{f}\NormalTok{(x, y)}
\NormalTok{y }\OperatorTok{%>%}\StringTok{ }\KeywordTok{f}\NormalTok{(x, ., z) =}\StringTok{ }\KeywordTok{f}\NormalTok{(x, y, z )}
\end{Highlighting}
\end{Shaded}

It is an operator from \texttt{magrittr} which can be really beneficial.
Look at the following code. Can you tell me what it does?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ave_exp <-}\StringTok{ }\KeywordTok{filter}\NormalTok{( }
  \KeywordTok{summarise}\NormalTok{(}
    \KeywordTok{group_by}\NormalTok{( }
      \KeywordTok{filter}\NormalTok{(}
\NormalTok{        sim.dat, }
        \OperatorTok{!}\KeywordTok{is.na}\NormalTok{(income)}
\NormalTok{      ), }
\NormalTok{      segment}
\NormalTok{    ), }
    \DataTypeTok{ave_online_exp =} \KeywordTok{mean}\NormalTok{(online_exp), }
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{()}
\NormalTok{  ), }
\NormalTok{  n }\OperatorTok{>}\StringTok{ }\DecValTok{200}
\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Now look at the identical code using ``\texttt{\%\textgreater{}\%}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ave_exp <-}\StringTok{ }\NormalTok{sim.dat }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(income)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{group_by}\NormalTok{(segment) }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{summarise}\NormalTok{( }
   \DataTypeTok{ave_online_exp =} \KeywordTok{mean}\NormalTok{(online_exp), }
   \DataTypeTok{n =} \KeywordTok{n}\NormalTok{() ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n }\OperatorTok{>}\StringTok{ }\DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Isn't it much more straightforward now? Let's read it:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Delete observations from \texttt{sim.dat} with missing income values
\item
  Group the data from step 1 by variable \texttt{segment}
\item
  Calculate mean of online expense for each segment and save the result
  as a new variable named \texttt{ave\_online\_exp}
\item
  Calculate the size of each segment and saved it as a new variable
  named \texttt{n}
\item
  Get segments with size larger than 200
\end{enumerate}

You can use \texttt{distinct()} to delete duplicated rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{distinct}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\texttt{sample\_frac()} will randomly select some rows with a specified
percentage. \texttt{sample\_n()} can randomly select rows with a
specified number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{sample_frac}\NormalTok{(sim.dat, }\FloatTok{0.5}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{) }
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{sample_n}\NormalTok{(sim.dat, }\DecValTok{10}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\texttt{slice()} will select rows by position:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{slice}\NormalTok{(sim.dat, }\DecValTok{10}\OperatorTok{:}\DecValTok{15}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

It is equivalent to \texttt{sim.dat{[}10:15,{]}}.

\texttt{top\_n()} will select the order top n entries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{top_n}\NormalTok{(sim.dat,}\DecValTok{2}\NormalTok{,income)}
\end{Highlighting}
\end{Shaded}

If you want to select columns instead of rows, you can use
\texttt{select()}. The following are some sample codes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select by column name}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat,income,age,store_exp)}

\CommentTok{# select columns whose name contains a character string}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{contains}\NormalTok{(}\StringTok{"_"}\NormalTok{))}

\CommentTok{# select columns whose name ends with a character string}
\CommentTok{# similar there is "starts_with"}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"e"}\NormalTok{))}

\CommentTok{# select columns Q1,Q2,Q3,Q4 and Q5}
\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{num_range}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)) }

\CommentTok{# select columns whose names are in a group of names}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{one_of}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{)))}

\CommentTok{# select columns between age and online_exp}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, age}\OperatorTok{:}\NormalTok{online_exp)}

\CommentTok{# select all columns except for age}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, }\OperatorTok{-}\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

\textbf{Summarize}

A standard marketing problem is customer segmentation. It usually starts
with designing survey and collecting data. Then run a cluster analysis
on the data to get customer segments. Once we have different segments,
the next is to understand how each group of customer look like by
summarizing some key metrics. For example, we can do the following data
aggregation for different segments of clothes customers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(segment)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Age=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(age)),}\DecValTok{0}\NormalTok{),}
      \DataTypeTok{FemalePct=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(gender}\OperatorTok{==}\StringTok{"Female"}\NormalTok{),}\DecValTok{2}\NormalTok{),}
      \DataTypeTok{HouseYes=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(house}\OperatorTok{==}\StringTok{"Yes"}\NormalTok{),}\DecValTok{2}\NormalTok{),}
      \DataTypeTok{store_exp=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(store_exp),}\DataTypeTok{trim=}\FloatTok{0.1}\NormalTok{),}\DecValTok{0}\NormalTok{),}
      \DataTypeTok{online_exp=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(online_exp),}\DecValTok{0}\NormalTok{),}
      \DataTypeTok{store_trans=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(store_trans),}\DecValTok{1}\NormalTok{),}
      \DataTypeTok{online_trans=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(online_trans),}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now, let's peel the onion in order.

The first line \texttt{sim.dat} is easy. It is the data you want to work
on. The second line \texttt{group\_by(segment)} tells R that in the
following steps you want to summarise by variable \texttt{segment}. Here
we only summarize data by one categorical variable, but you can group by
multiple variables, such as \texttt{group\_by(segment,\ house)}. The
third argument \texttt{summarise} tells R the manipulation(s) to do.
Then list the exact actions inside \texttt{summarise()} . For example,
\texttt{Age=round(mean(na.omit(age)),0)} tell R the following things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the mean of column \texttt{age} ignoring missing value for
  each customer segment
\item
  Round the result to the specified number of decimal places
\item
  Store the result in a new variable named \texttt{Age}
\end{enumerate}

The rest of the command above is similar. In the end, we calculate the
following for each segment:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{Age}: average age for each segment
\item
  \texttt{FemalePct}: percentage for each segment
\item
  \texttt{HouseYes}: percentage of people who own a house
\item
  \texttt{stroe\_exp}: average expense in store
\item
  \texttt{online\_exp}: average expense online
\item
  \texttt{store\_trans}: average times of transactions in the store
\item
  \texttt{online\_trans}: average times of online transactions
\end{enumerate}

There is a lot of information you can extract from those simple
averages.

\begin{itemize}
\item
  Conspicuous: average age is about 40. It is a group of middle-age
  wealthy people. 1/3 of them are female, and 2/3 are male. They buy
  regardless the price. Almost all of them own house (0.86). It makes us
  wonder what is wrong with the rest 14\%?
\item
  Price: They are older people with average age 60. Nearly all of them
  own a house(0.94). They are less likely to purchase online
  (store\_trans=6 while online\_trans=3). It is the only group that is
  less likely to buy online.
\item
  Quality: The average age is 35. They are not way different with
  Conspicuous regarding age. But they spend much less. The percentages
  of male and female are similar. They prefer online shopping. More than
  half of them don't own a house (0.66).
\item
  Style: They are young people with average age 24. The majority of them
  are female (0.81). Most of them don't own a house (0.73). They are
  very likely to be digital natives and prefer online shopping.
\end{itemize}

You may notice that Style group purchase more frequently online
(\texttt{online\_trans}) but the expense (\texttt{online\_exp}) is not
higher. It makes us wonder what is the average expense each time, so you
have a better idea about the price range of the group.

The analytical process is aggregated instead of independent steps. The
current step will shed new light on what to do next. Sometimes you need
to go back to fix something in the previous steps. Let's check average
one-time online and instore purchase amounts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat}\OperatorTok{%>%}
\KeywordTok{group_by}\NormalTok{(segment)}\OperatorTok{%>%}
\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg_online=}\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(online_exp)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(online_trans),}\DecValTok{2}\NormalTok{),}
    \DataTypeTok{avg_store=}\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(store_exp)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(store_trans),}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Price group has the lowest averaged one-time purchase. The Conspicuous
group will pay the highest price. When we build customer profile in real
life, we will also need to look at the survey summarization. You may be
surprised how much information simple data manipulations can provide.

Another comman task is to check which column has missing values. It
requires the program to look at each column in the data. In this case
you can use \texttt{summarise\_all}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# apply function anyNA() to each column}
\CommentTok{# you can also assign a function vector such as: c("anyNA","is.factor")}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{summarise_all}\NormalTok{(sim.dat, }\KeywordTok{funs_}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"anyNA"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

The above code returns a vector indicating if there is any value missing
in each column.

\textbf{Create new variable}

There are often situations where you need to create new variables. For
example, adding online and store expense to get total expense. In this
case, you will apply \textbf{window function} to the columns and return
a column with the same length. \texttt{mutate()} can do it for you and
append one or more new columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(sim.dat, }\DataTypeTok{total_exp =}\NormalTok{ store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp)}
\end{Highlighting}
\end{Shaded}

The above code sums up two columns and appends the result
(\texttt{total\_exp}) to \texttt{sim.dat}. Another similar function is
\texttt{transmute()}. The difference is that \texttt{transmute()} will
delete the original columns and only keep the new ones.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{transmute}\NormalTok{(sim.dat, }\DataTypeTok{total_exp =}\NormalTok{ store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp) }
\end{Highlighting}
\end{Shaded}

\textbf{Merge}

Similar to SQL, there are different joins in \texttt{dplyr}. We create
two baby data sets to show how the functions work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x<-}\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{ID=}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{),}\DataTypeTok{x1=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))))}
\NormalTok{(y<-}\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{ID=}\KeywordTok{c}\NormalTok{(}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{,}\StringTok{"D"}\NormalTok{),}\DataTypeTok{y1=}\KeywordTok{c}\NormalTok{(T,T,F))))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# join to the left}
\CommentTok{# keep all rows in x}
\KeywordTok{left_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get rows matched in both data sets}
\KeywordTok{inner_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get rows in either data set}
\KeywordTok{full_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# filter out rows in x that can be matched in y }
\CommentTok{# it doesn't bring in any values from y }
\KeywordTok{semi_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the opposite of  semi_join()}
\CommentTok{# it gets rows in x that cannot be matched in y}
\CommentTok{# it doesn't bring in any values from y}
\KeywordTok{anti_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are other functions(\texttt{intersect()}, \texttt{union()} and
\texttt{setdiff()}). Also the data frame version of \texttt{rbind} and
\texttt{cbind} which are \texttt{bind\_rows()} and \texttt{bind\_col()}.
We are not going to go through them all. You can try them yourself. If
you understand the functions we introduced so far. It should be easy for
you to figure out the rest.

\section{Tidy and Reshape Data}\label{tidy-and-reshape-data}

``Tidy data'' represent the information from a dataset as data frames
where each row is an observation, and each column contains the values of
a variable (i.e., an attribute of what we are observing). Depending on
the situation, the requirements on what to present as rows and columns
may change. To make data easy to work with for the problem at hand, in
practice, we often need to convert data between the ``wide'' and the
``long'' format. The process feels like kneading the dough.

There are two commonly used packages for this kind of manipulations:
\texttt{tidyr} and \texttt{reshape2}. We will show how to tidy and
reshape data using the two packages. By comparing the functions to show
how they overlap and where they differ.

\subsection{\texorpdfstring{\texttt{reshape2}
package}{reshape2 package}}\label{reshape2-package}

It is a reboot of the previous package \texttt{reshape}. Take a baby
subset of our exemplary clothes consumers data to illustrate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(sdat<-sim.dat[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

For the above data \texttt{sdat}, what if we want to have a variable
indicating the purchasing channel (i.e.~online or in-store) and another
column with the corresponding expense amount? Assume we want to keep the
rest of the columns the same. It is a task to change data from ``wide''
to ``long''. There are two general ways to shape data:

\begin{itemize}
\tightlist
\item
  Use \texttt{melt()} to convert an object into a molten data frame,
  i.e., from wide to long
\item
  Use \texttt{dcast()} to cast a molten data frame into the shape you
  want, i.e., from long to wide
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reshape2)}
\NormalTok{(mdat <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(sdat, }\DataTypeTok{measure.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{),}
              \DataTypeTok{variable.name =} \StringTok{"Channel"}\NormalTok{,}
              \DataTypeTok{value.name =} \StringTok{"Expense"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

You melted the data frame \texttt{sdat} by two variables:
\texttt{store\_exp} and \texttt{online\_exp}
(\texttt{measure.vars=c("store\_exp","online\_exp")}). The new variable
name is \texttt{Channel} set by command
\texttt{variable.name\ =\ "Channel"}. The value name is \texttt{Expense}
set by command \texttt{value.name\ =\ "Expense"}.

You can run a regression to study the effect of purchasing channel as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Here we use all observations from sim.dat}
\CommentTok{# Don't show result here}
\NormalTok{mdat<-}\KeywordTok{melt}\NormalTok{(sim.dat[,}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{], }\DataTypeTok{measure.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{),}
            \DataTypeTok{variable.name =} \StringTok{"Channel"}\NormalTok{,}
              \DataTypeTok{value.name =} \StringTok{"Expense"}\NormalTok{)}
\NormalTok{fit<-}\KeywordTok{lm}\NormalTok{(Expense}\OperatorTok{~}\NormalTok{gender}\OperatorTok{+}\NormalTok{house}\OperatorTok{+}\NormalTok{income}\OperatorTok{+}\NormalTok{Channel}\OperatorTok{+}\NormalTok{age,}\DataTypeTok{data=}\NormalTok{mdat)}
\KeywordTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

You can \texttt{melt()} list, matrix, table too. The syntax is similar,
and we won't go through every situation. Sometimes we want to convert
the data from ``long'' to ``wide''. For example, \textbf{you want to
compare the online and in-store expense between male and female based on
the house ownership. }

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dcast}\NormalTok{(mdat, house }\OperatorTok{+}\StringTok{ }\NormalTok{gender }\OperatorTok{~}\StringTok{ }\NormalTok{Channel, sum)}
\end{Highlighting}
\end{Shaded}

In the above code, what is the left side of \texttt{\textasciitilde{}}
are variables that you want to group by. The right side is the variable
you want to spread as columns. It will use the column indicating value
from \texttt{melt()} before. Here is ``\texttt{Expense}'' .

\subsection{\texorpdfstring{\texttt{tidyr}
package}{tidyr package}}\label{tidyr-package}

The other package that will do similar manipulations is \texttt{tidyr}.
Let's get a subset to illustrate the usage.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{# practice functions we learnt before}
\NormalTok{sdat<-sim.dat[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,]}\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(age,gender,store_exp,store_trans)}
\NormalTok{sdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\texttt{gather()} function in \texttt{tidyr} is analogous to
\texttt{melt()} in \texttt{reshape2}. The following code will do the
same thing as we did before using \texttt{melt()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr)}
\NormalTok{msdat<-tidyr}\OperatorTok{::}\KeywordTok{gather}\NormalTok{(sdat,}\StringTok{"variable"}\NormalTok{,}\StringTok{"value"}\NormalTok{,store_exp,store_trans)}
\NormalTok{msdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Or if we use the pipe operation, we can write the above code as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdat}\OperatorTok{%>%}\KeywordTok{gather}\NormalTok{(}\StringTok{"variable"}\NormalTok{,}\StringTok{"value"}\NormalTok{,store_exp,store_trans)}
\end{Highlighting}
\end{Shaded}

It is identical with the following code using \texttt{melt()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{melt}\NormalTok{(sdat, }\DataTypeTok{measure.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"store_trans"}\NormalTok{),}
            \DataTypeTok{variable.name =} \StringTok{"variable"}\NormalTok{,}
              \DataTypeTok{value.name =} \StringTok{"value"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The opposite operation to \texttt{gather()} is \texttt{spread()}. The
previous one stacks columns and the latter one spread the columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{spread}\NormalTok{(variable,value)}
\end{Highlighting}
\end{Shaded}

Another pair of functions that do opposite manipulations are
\texttt{separate()} and \texttt{unite()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sepdat<-}\StringTok{ }\NormalTok{msdat }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(variable,}\KeywordTok{c}\NormalTok{(}\StringTok{"Source"}\NormalTok{,}\StringTok{"Type"}\NormalTok{))}
\NormalTok{sepdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

You can see that the function separates the original column
``\texttt{variable}'' to two new columns ``\texttt{Source}'' and
``\texttt{Type}''. You can use \texttt{sep=} to set the string or
regular expression to separate the column. By default, it is
``\texttt{\_}''.

The \texttt{unite()} function will do the opposite: combining two
columns. It is the generalization of \texttt{paste()} to a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sepdat }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unite}\NormalTok{(}\StringTok{"variable"}\NormalTok{,Source,Type,}\DataTypeTok{sep=}\StringTok{"_"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The reshaping manipulations may be the trickiest part. You have to
practice a lot to get familiar with those functions. Unfortunately,
there is no shortcut.

\chapter{Model Tuning Strategy}\label{model-tuning-strategy}

When training a machine learning model, there are many decisions to
make. For example, when training a random forest, you need to decide the
number of trees and the number of variables at each node. For lasso
method, you need to determine the penalty parameter. There may be
standard settings for some of the parameters, but it's unlikely to guess
the right values for all of these correctly. Other than that, making
good choices on how you split the data into training and testing sets
can make a huge difference in helping you find a high-performance model
efficiently.

This chapter will illustrate the practical aspects of model tuning. We
will talk about different types of model error, sources of model error,
hyperparameter tuning, how to set up your data and how to make sure your
model implementation is correct. In practice applying machine learning
is a highly iterative process.

\section{Systematic Error and Random
Error}\label{systematic-error-and-random-error}

Assume \(\mathbf{X}\) is \(n \times p\) observation matrix and
\(\mathbf{y}\) is response variable, we have:

\[\mathbf{y}=f(\mathbf{X})+\mathbf{\epsilon}\]

where \(\mathbf{\epsilon}\) is the random error with a mean of zero. The
function \(f(\cdot)\) is our modeling target, which represents the
information in the response variable that predictors can explain. The
main goal of estimating \(f(\cdot)\) is inference or prediction, or
sometimes both. In general, there is a trade-off between flexibility and
interpretability of the model. So data scientists need to comprehend the
delicate balance between these two.

Depending on the modeling purposes, the requirement for interpretability
varies. If the prediction is the only goal, then as long as the
prediction is accurate enough, the interpretability is not under
consideration. In this case, people can use ``black box'' model, such as
random forest, boosting tree, neural network and so on. These models are
very flexible but nearly impossible to explain. Their accuracy is
usually higher on the training set, but not necessary when it predicts.
It is not surprising since those models have a huge number of parameters
and high flexibility that they can ``memorize'' the entire training
data. A paper by Chiyuan Zhang et al. in 2017 pointed out that ``Deep
neural networks (even just two-layer net) easily fit random labels''
\citep{rethinkDL}. The traditional forms of regularization, such as
weight decay, dropout, and data augmentation, fail to control
generalization error. It poses a conceptual challenge to statistical
theory and also calls our attention when we use such black-box models.

There are two kinds of application problems: complete information
problem and incomplete information problem. The complete information
problem has all the information you need to know the correct response.
Take the famous cat recognition, for example, all the information you
need to identify a cat is in the picture. In this situation, the
algorithm that penetrates the data the most wins. There are some other
similar problems such as the self-driving car, chess game, facial
recognition and speech recognition. But in most of the data science
applications, the information is incomplete. If you want to know whether
a customer is going to purchase again or not, it is unlikely to have
360-degree of the customer's information. You may have their historical
purchasing record, discounts and service received. But you don't know if
the customer sees your advertisement, or has a friend recommends
competitor's product, or encounters some unhappy purchasing experience
somewhere. There could be a myriad of factors that will influence the
customer's purchase decision while what you have as data is only a small
part. To make things worse, in many cases, you don't even know what you
don't know. Deep learning doesn't have any advantage in solving those
problems. Instead, some parametric models often work better in this
situation. You will comprehend this more after learning the different
types of model error. Assume we have \(\hat{f}\) which is an estimator
of \(f\). Then we can further get
\(\mathbf{\hat{y}}=\hat{f}(\mathbf{X})\). The predicted error is divided
into two parts, systematic error, and random error:

\[E(\mathbf{y}-\hat{\mathbf{y}})^{2}=E[f(\mathbf{X})+\mathbf{\epsilon}-\hat{f}(\mathbf{X})]^{2}=\underset{\text{(1)}}{\underbrace{E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2}}}+\underset{\text{(2)}}{\underbrace{Var(\mathbf{\epsilon})}}
\label{eq:error}\]

It is also called Mean Square Error (MSE) where (1) is the systematic
error. It exists because \(\hat{f}\) usually does not entirely describe
the ``systematic relation'' between X and y which refers to the stable
relationship that exists across different samples or time. Model
improvement can help reduce this kind of error; (2) is the random error
which represents the part of y that cannot be explained by X. A more
complex model does not reduce the error. There are three reasons for
random error:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the current sample is not representative, so the pattern in one sample
  set does not generalize to a broader scale.
\item
  The information is incomplete. In other words, you don't have all
  variables needed to explain the response.
\item
  Measurement error in the variables.
\end{enumerate}

Deep learning has significant success solving problems with complete
information and usually low measurement error. As mentioned before, in a
task like image recognition, all you need are the pixels in the
pictures. So in deep learning applications, increasing the sample size
can improve the model performance significantly. But it may not perform
well in problems with incomplete information. The biggest problem with
the black-box model is that it fits random error, i.e., over-fitting.
The notable feature of random error is that it varies over different
samples. So one way to determine whether overfitting happens is to
reserve a part of the data as the test set and then check the
performance of the trained model on the test data. Note that overfitting
is a general problem from which any model could suffer. However, since
black-box models usually have a large number of parameters, it is much
more suspectable to over-fitting.

\begin{figure}
\centering
\includegraphics{images/ModelError.png}
\caption{Types of Model Error}
\end{figure}

The systematic error can be further decomposed as:

\[
\begin{array}{ccc}
E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2} & = & E\left(f(\mathbf{X})-E[\hat{f}(\mathbf{X})]+E[\hat{f}(\mathbf{X})]-\hat{f}(\mathbf{X})\right)^{2}\\
 & = & E\left(E[\hat{f}(\mathbf{X})]-f(\mathbf{X})\right)^{2}+E\left(\hat{f}(\mathbf{X})-E[\hat{f}(\mathbf{X})]\right)^{2}\\
 & = & [Bias(\hat{f}(\mathbf{X}))]^{2}+Var(\hat{f}(\mathbf{X}))
\end{array}
\]

The systematic error consists of two parts,
\(Bias(\hat{f}(\mathbf{X}))\) and \(Var (\hat{f}(\mathbf{X}))\). To
minimize the systematic error, we need to minimize both. The bias
represents the error caused by the model's approximation of the reality,
i.e., systematic relation, which may be very complex. For example,
linear regression assumes a linear relationship between the predictors
and the response, but rarely is there a perfect linear relationship in
real life. So linear regression is more likely to have a high bias.

To explore bias and variance, let's begin with a simple simulation. We
will simulate a data with a non-linear relationship and fit different
models on it. An intuitive way to show these is to compare the plots of
various models.

The code below simulates one predictor (\texttt{x}) and one response
variable (\texttt{fx}). The relationship between \texttt{x} and
\texttt{fx} is non-linear.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{source}\NormalTok{(}\KeywordTok{ids_url}\NormalTok{(}\StringTok{'R/multiplot.r'}\NormalTok{))}
\CommentTok{# randomly simulate some non-linear samples}
\NormalTok{x =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{pi}
\NormalTok{e =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(x), }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =} \FloatTok{0.2}\NormalTok{)}
\NormalTok{fx <-}\StringTok{ }\KeywordTok{sin}\NormalTok{(x) }\OperatorTok{+}\StringTok{ }\NormalTok{e }\OperatorTok{+}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(x)}
\NormalTok{dat =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(x, fx)}
\end{Highlighting}
\end{Shaded}

Then fit a simple linear regression on these data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot fitting result}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(dat, }\KeywordTok{aes}\NormalTok{(x, fx)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/linearbias-1} 

}

\caption{High bias model}\label{fig:linearbias}
\end{figure}

Despite a large sample size, trained linear regression cannot describe
the relationship very well. In other words, in this case, the model has
a high bias (Fig. \ref{fig:linearbias}). People also call it
underfitting.

Since the estimated parameters will be somewhat different for different
samples, there is the variance of estimates. Intuitively, it gives you
some sense that if we fit the same model with different samples
(presumably, they are from the same population), how much will the
estimates change. Ideally, the change is trivial. For high variance
models, small changes in the training data result in very different
estimates. In general, a model with high flexibility also has high
variance., such as the CART tree, and the initial boosting method. To
overcome that problem, the Random Forest and Gradient Boosting Model aim
to reduce the variance by summarizing the results obtained from
different samples.

Let's fit the above data using a smoothing method which is highly
flexible and can fit the current data tightly:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(dat, }\KeywordTok{aes}\NormalTok{(x, fx)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/linearvar-1} 

}

\caption{High variance model}\label{fig:linearvar}
\end{figure}

The resulting plot (Fig. \ref{fig:linearvar}) indicates the smoothing
method fit the data much better so it has a much smaller bias. However,
this method has a high variance. If we simulate different subsets of the
sample, the result curve will change significantly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2016}\NormalTok{)}
\CommentTok{# sample part of the data to fit model sample 1}
\NormalTok{idx1 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat1 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x1 =}\NormalTok{ x[idx1], }\DataTypeTok{fx1 =}\NormalTok{ fx[idx1])}
\NormalTok{p1 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat1, }\KeywordTok{aes}\NormalTok{(x1, fx1)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 2}
\NormalTok{idx2 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat2 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x2 =}\NormalTok{ x[idx2], }\DataTypeTok{fx2 =}\NormalTok{ fx[idx2])}
\NormalTok{p2 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat2, }\KeywordTok{aes}\NormalTok{(x2, fx2)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 3}
\NormalTok{idx3 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat3 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x3 =}\NormalTok{ x[idx3], }\DataTypeTok{fx3 =}\NormalTok{ fx[idx3])}
\NormalTok{p3 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat3, }\KeywordTok{aes}\NormalTok{(x3, fx3)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 4}
\NormalTok{idx4 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat4 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x4 =}\NormalTok{ x[idx4], }\DataTypeTok{fx4 =}\NormalTok{ fx[idx4])}
\NormalTok{p4 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat4, }\KeywordTok{aes}\NormalTok{(x4, fx4)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\KeywordTok{multiplot}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{cols =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-8-1.pdf}

The fitted lines (blue) change over different samples which means it has
high variance. People also call it overfitting. Fitting the linear model
using the same four subsets, the result barely changes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat1, }\KeywordTok{aes}\NormalTok{(x1, fx1)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p2 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat2, }\KeywordTok{aes}\NormalTok{(x2, fx2)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p3 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat3, }\KeywordTok{aes}\NormalTok{(x3, fx3)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p4 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat4, }\KeywordTok{aes}\NormalTok{(x4, fx4)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{multiplot}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{cols =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-9-1.pdf}

In general, the variance (\(Var(\hat{f}(\mathbf{X}))\))
\textbf{increases} and the bias (\(Bias(\hat{f}(\mathbf{X}))\))
\textbf{decreases} as the model flexibility increases. Variance and bias
together determine the systematic error. As we increase the flexibility
of the model, at first the rate at which \(Bias(\hat{f}(\mathbf{X}))\)
decreases is faster than \(Var (\hat{f} (\mathbf{X}))\), so the MSE
decreases. However, to some degree, higher flexibility has little effect
on \(Bias(\hat{f}(\mathbf{X}))\) but \(Var(\hat{f} (\mathbf{X}))\)
increases significantly, so the MSE increases.

\subsection{Measurement Error in the
Response}\label{measurement-error-in-the-response}

The measurement error in the response contributes to the random error
(\(\mathbf{\epsilon}\)). This part of the error is irreducible if you
change the data collection mechanism, and so it makes the root mean
square error (RMSE) and \(R^2\) have the corresponding upper and lower
limits. RMSE and \(R^2\) are commonly used performance measures for the
regression model which we will talk in more detail later. Therefore, the
random error term not only represents the part of fluctuations the model
cannot explain but also contains measurement error in the response
variables. Section 20.2 of Applied Predictive Modeling \citep{APM} has
an example that shows the effect of the measurement error in the
response variable on the model performance (RMSE and \(R^2\)).

The authors increased the error in the response proportional to a base
level error which was gotten using the original data without introducing
extra noise. Then fit a set of models repeatedly using the
``contaminated'' data sets to study the change of \(RMSE\) and \(R^2\)
as the level of noise. Here we use clothing consumer data for a similar
illustration. Suppose many people do not want to disclose their income
and so we need to use other variables to establish a model to predict
income. We set up the following model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv "}\NormalTok{)}
\NormalTok{ymad <-}\StringTok{ }\KeywordTok{mad}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{income))}
\CommentTok{# calculate z-score}
\NormalTok{zs <-}\StringTok{ }\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{income)))}\OperatorTok{/}\NormalTok{ymad}
\CommentTok{# which(na.omit(zs>3.5)): identify outliers which(is.na(zs)):}
\CommentTok{# identify missing values}
\NormalTok{idex <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(zs }\OperatorTok{>}\StringTok{ }\FloatTok{3.5}\NormalTok{)), }\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(zs)))}
\CommentTok{# delete rows with outliers and missing values}
\NormalTok{sim.dat <-}\StringTok{ }\NormalTok{sim.dat[}\OperatorTok{-}\NormalTok{idex, ]}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(income }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }\NormalTok{online_trans, }
    \DataTypeTok{data =}\NormalTok{ sim.dat)}
\end{Highlighting}
\end{Shaded}

The output shows that without additional noise, the root mean square
error (RMSE) of the model is 29567, \(R^2\) is 0.6.

Let's add various degrees of noise (0 to 3 times the RMSE) to the
variable \texttt{income}:

\[ RMSE \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noise <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DecValTok{7} \OperatorTok{*}\StringTok{ }\KeywordTok{nrow}\NormalTok{(sim.dat)), }\DataTypeTok{nrow =} \KeywordTok{nrow}\NormalTok{(sim.dat), }
    \DataTypeTok{ncol =} \DecValTok{7}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(sim.dat)) \{}
\NormalTok{    noise[i, ] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{7}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{), }\KeywordTok{summary}\NormalTok{(fit)}\OperatorTok{$}\NormalTok{sigma }\OperatorTok{*}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }
        \DecValTok{3}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.5}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We then examine the effect of noise intensity on \(R^2\) for models with
different complexity. The models with complexity from low to high are:
ordinary linear regression, partial least square regression(PLS),
multivariate adaptive regression spline (MARS), support vector machine
(SVM, the kernel function is radial basis function), and random forest.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit ordinary linear regression}
\NormalTok{rsq_linear <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(withnoise }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat)}
\NormalTok{    rsq_linear[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(fit0)}\OperatorTok{$}\NormalTok{adj.r.squared}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

PLS is a method of linearizing nonlinear relationships through hidden
layers. It is similar to the principal component regression (PCR),
except that PCR does not take into account the information of the
dependent variable when selecting the components, and its purpose is to
find the linear combinations (i.e., unsupervised) that capture the most
variance of the independent variables. When the independent variables
and response variables are related, PCR can well identify the systematic
relationship between them. However, when there exist independent
variables not associated with response variable, it will undermine PCR's
performance. And PLS maximizes the linear combination of dependencies
with the response variable. In the current case, the more complicated
PLS does not perform better than simple linear regression.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# pls: conduct PLS and PCR}
\KeywordTok{library}\NormalTok{(pls)}
\NormalTok{rsq_pls <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# fit PLS}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{plsr}\NormalTok{(withnoise }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat)}
\NormalTok{    rsq_pls[i] <-}\StringTok{ }\KeywordTok{max}\NormalTok{(}\KeywordTok{drop}\NormalTok{(}\KeywordTok{R2}\NormalTok{(fit0, }\DataTypeTok{estimate =} \StringTok{"train"}\NormalTok{, }\DataTypeTok{intercept =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{$}\NormalTok{val))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# earth: fit mars}
\KeywordTok{library}\NormalTok{(earth)}
\NormalTok{rsq_mars <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{earth}\NormalTok{(withnoise }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat)}
\NormalTok{    rsq_mars[i] <-}\StringTok{ }\NormalTok{fit0}\OperatorTok{$}\NormalTok{rsq}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# caret: awesome package for tuning predictive model}
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{rsq_svm <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# Need some time to run}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    idex <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{income))}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    trainX <-}\StringTok{ }\NormalTok{sim.dat[, }\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{, }\StringTok{"online_exp"}\NormalTok{, }\StringTok{"store_trans"}\NormalTok{, }
        \StringTok{"online_trans"}\NormalTok{)]}
\NormalTok{    trainY <-}\StringTok{ }\NormalTok{withnoise}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(trainX, trainY, }\DataTypeTok{method =} \StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{tuneLength =} \DecValTok{15}\NormalTok{, }
        \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{))}
\NormalTok{    rsq_svm[i] <-}\StringTok{ }\KeywordTok{max}\NormalTok{(fit0}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{Rsquared)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# randomForest: random forest model}
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{rsq_rf <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# ntree=500 number of trees na.action = na.omit ignore}
\CommentTok{# missing value}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(withnoise }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat, }\DataTypeTok{ntree =} \DecValTok{500}\NormalTok{, }
        \DataTypeTok{na.action =}\NormalTok{ na.omit)}
\NormalTok{    rsq_rf[i] <-}\StringTok{ }\KeywordTok{tail}\NormalTok{(fit0}\OperatorTok{$}\NormalTok{rsq, }\DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\KeywordTok{library}\NormalTok{(reshape2)}
\NormalTok{rsq <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{Noise =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\DecValTok{3}\NormalTok{), }
\NormalTok{    rsq_linear, rsq_pls, rsq_mars, rsq_svm, rsq_rf))}
\NormalTok{rsq <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(rsq, }\DataTypeTok{id.vars =} \StringTok{"Noise"}\NormalTok{, }\DataTypeTok{measure.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"rsq_linear"}\NormalTok{, }
    \StringTok{"rsq_pls"}\NormalTok{, }\StringTok{"rsq_mars"}\NormalTok{, }\StringTok{"rsq_svm"}\NormalTok{, }\StringTok{"rsq_rf"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}







\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ rsq, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Noise, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{group =}\NormalTok{ variable, }
    \DataTypeTok{colour =}\NormalTok{ variable)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"R2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/error-1} 

}

\caption{Test set \(R^2\) profiles for income models when
measurement system noise increases. \texttt{rsq\_linear}: linear
regression, \texttt{rsq\_pls}: Partial Least Square, \texttt{rsq\_mars}:
Multiple Adaptive Regression Spline Regression, \texttt{rsq\_svm}:
Support Vector Machine，\texttt{rsq\_rf}: Random Forest}\label{fig:error}
\end{figure}

Fig. \ref{fig:error} shows that:

All model performance decreases sharply with increasing noise intensity.
To better anticipate model performance, it helps to understand the way
variable is measured. It is something need to make clear at the
beginning of an analytical project. A data scientist should be aware of
the quality of the data in the database. For data from the clients, it
is an important to understand the quality of the data by communication.

More complex model is not necessarily better. The best model in this
situation is MARS, not random forests or SVM. Simple linear regression
and PLS perform the worst when noise is low. MARS is more complicated
than the linear regression and PLS, but it is simpler and easier to
explain than random forest and SVM.

When noise increases to a certain extent, the potential structure
becomes vaguer, and complex random forest model starts to fail. When the
systematic measurement error is significant, a more straightforward but
not naive model may be a better choice. It is always a good practice to
try different models, and select the simplest model in the case of
similar performance. Model evaluation and selection represent the career
``maturity'' of a data scientist.

\subsection{Measurement Error in the Independent
Variables}\label{measurement-error-in-the-independent-variables}

The traditional statistical model usually assumes that the measurement
of the independent variable has no error which is not possible in
practice. Considering the error in the independent variables is
necessary. The impact of the error depends on the following factors: (1)
the magnitude of the randomness; (2) the importance of the corresponding
variable in the model, and (3) the type of model used. Use variable
\texttt{online\_exp} as an example. The approach is similar to the
previous section. Add varying degrees of noise and see its impact on the
model performance. We add the following different levels of noise (0 to
3 times the standard deviation) to\texttt{online\_exp}:

\[\sigma_{0} \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0)\]

where \(\sigma_{0}\) is the standard error of \texttt{online\_exp}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noise<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{7}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(sim.dat)),}\DataTypeTok{nrow=}\KeywordTok{nrow}\NormalTok{(sim.dat),}\DataTypeTok{ncol=}\DecValTok{7}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(sim.dat))\{}
\NormalTok{noise[i,]<-}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{7}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{),}\KeywordTok{sd}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{online_exp)}\OperatorTok{*}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.5}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Likewise, we examine the effect of noise intensity on different models
(\(R^2\)). The models with complexity from low to high are: ordinary
linear regression, partial least square regression(PLS), multivariate
adaptive regression spline (MARS), support vector machine (SVM, the
Kernel function is radial basis function), and random forest. The code
is similar as before so not shown here.








\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/errorvariable-1} 

}

\caption{Test set \(R^2\) profiles for income models when
noise in \texttt{online\_exp} increases. \texttt{rsq\_linear} : linear
regression, \texttt{rsq\_pls} : Partial Least Square,
\texttt{rsq\_mars}: Multiple Adaptive Regression Spline Regression,
\texttt{rsq\_svm}: Support Vector Machine，\texttt{rsq\_rf}: Random
Forest}\label{fig:errorvariable}
\end{figure}

Comparing Fig. \ref{fig:errorvariable} and Fig. \ref{fig:error}, the
influence of the two types of error is very different. The error in
response cannot be overcome for any model, but it is not the case for
the independent variables. Imagine an extreme case, if
\texttt{online\_exp} is completely random, that is, no information in
it, the impact on the performance of random forest and support vector
machine is marginal. Linear regression and PLS still perform similarly.
With the increase of noise, the performance starts to decline faster. To
a certain extent, it becomes steady. In general, if an independent
variable contains error, other variables associated with it can
compensate to some extent.

\section{Data Splitting and
Resampling}\label{data-splitting-and-resampling}

Those highly adaptable models can model complex relationships. However,
they tend to overfit which leads to the poor prediction by learning too
much from the data. It means that the model is susceptible to the
specific sample used to fit it. When future data is not exactly like the
past data, the model prediction may have big mistakes. A simple model
like ordinary linear regression tends instead to underfit which leads to
a bad prediction by learning too little from the data. It systematically
over-predicts or under-predicts the data regardless of how well future
data resemble past data. Without evaluating models, the modeler will not
know about the problem before the future samples. Data splitting and
resampling are fundamental techniques to build sound models for
prediction.

\subsection{Data Splitting}\label{data-splitting}

\emph{Data splitting} is to put part of the data aside as testing set
(or Hold-outs, out of bag samples) and use the rest for model training.
Training samples are also called in-sample. Model performance metrics
evaluated using in-sample are retrodictive, not predictive.

The traditional business intelligence usually handles data description.
Answer simple questions by querying and summarizing the data, such as:

\begin{itemize}
\tightlist
\item
  What is the monthly sales of a product in 2015?
\item
  What is the number of visits to our site in the past month?\\
\item
  What is the sales difference in 2015 for two different product
  designs?
\end{itemize}

There is no need to go through the tedious process of splitting the
data, tuning and testing model to answer questions of this kind.
Instead, people usually use as complete data as possible and then sum or
average the parts of interest.

Many models have parameters which cannot be directly estimated from the
data, such as \(\lambda\) in the lasso (penalty parameter), the number
of trees in the random forest. This type of model parameter is called
tuning parameter, and there is no analytical formula available to
calculate the optimized value. Tuning parameters often control the
complexity of the model. A poor choice can result in over-fitting or
under-fitting. A standard approach to estimate tuning parameters is
through cross-validation which is a data resampling approach.

To get a reasonable precision of the performance based on a single test
set, the size of the test set may need to be large. So a conventional
approach is to use a subset of samples to fit the model and use the rest
to evaluate model performance. This process will repeat multiple times
to get a performance profile. In that sense, resampling is based on
splitting. The general steps are:

\begin{itemize}
\tightlist
\item
  Define a set of candidate values for tuning parameter(s)

  \begin{itemize}
  \tightlist
  \item
    For each candidate value in the set

    \begin{itemize}
    \tightlist
    \item
      Resample data
    \item
      Fit model
    \item
      Predict hold-out
    \item
      Calculate performance
    \end{itemize}
  \end{itemize}
\item
  Aggregate the results
\item
  Determine the final tuning parameter
\item
  Refit the model with the entire data set
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.80000\textwidth]{images/ParameterTuningProcess.png}
\caption{Parameter Tuning Process}
\end{figure}

The above is an outline of the general procedure to tune parameters. Now
let's focus on the critical part of the process: data splitting.
Ideally, we should evaluate model using samples that were not used to
build or fine-tune the model. So it provides an unbiased sense of model
effectiveness. When the sample size is large, it is a good practice to
set aside part of the samples to evaluate the final model. People use
``training'' data to indicate samples used to fit or fine-tune the model
and ``test'' or ``validation'' data set is used to validate performance.

The first decision to make for data splitting is to decide the
proportion of data in the test set. There are two factors to consider
here: (1) sample size; (2) computation intensity. If the sample size is
large enough which is the most common situation according to my
experience, you can try to use 20\%, 30\% and 40\% of the data as the
test set, and see which one works the best. If the model is
computationally intense, then you may consider starting from a smaller
sample of data to train the model hence will have a higher portion of
data in the test set. Depending on how it performs, you may need to
increase the training set. If the sample size is small, you can use
cross-validation or bootstrap which is the topic in the next section.

The next decision is to decide which samples are in the test set. There
is a desire to make the training and test sets as similar as possible. A
simple way is to split data by random sampling which, however, does not
control for any of the data attributes, such as the percentage of the
retained customer in the data. So it is possible that the distribution
of outcomes is substantially different between the training and test
sets. There are three main ways to split the data that account for the
similarity of resulted data sets. We will describe the three approaches
using the clothing company customer data as examples.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Split data according to the outcome variable
\end{enumerate}

Assume the outcome variable is customer segment (column
\texttt{segment}) and we decide to use 80\% as training and 20\% test.
The goal is to make the proportions of the categories in the two sets as
similar as possible. The \texttt{createDataPartition()} function in
\texttt{caret} will return a balanced splitting based on assigned
variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{# set random seed to make sure reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{3456}\NormalTok{)}
\NormalTok{trainIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{segment, }\DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{times =} \DecValTok{1}\NormalTok{)}
\KeywordTok{head}\NormalTok{(trainIndex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Resample1
## [1,]         1
## [2,]         2
## [3,]         3
## [4,]         4
## [5,]         6
## [6,]         7
\end{verbatim}

The \texttt{list\ =\ FALSE} in the call to \texttt{createDataPartition}
is to return a data frame. The \texttt{times\ =\ 1} tells R how many
times you want to split the data. Here we only do it once, but you can
repeat the splitting multiple times. In that case, the function will
return multiple vectors indicating the rows to training/test. You can
set \texttt{times＝2} and rerun the above code to see the result. Then
we can use the returned indicator vector \texttt{trainIndex} to get
training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get training set}
\NormalTok{datTrain <-}\StringTok{ }\NormalTok{sim.dat[trainIndex, ]}
\CommentTok{# get test set}
\NormalTok{datTest <-}\StringTok{ }\NormalTok{sim.dat[}\OperatorTok{-}\NormalTok{trainIndex, ]}
\end{Highlighting}
\end{Shaded}

According to the setting, there are 800 samples in the training set and
200 in test set. Let's check the distribution of the two sets:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(plyr)}
\KeywordTok{ddply}\NormalTok{(datTrain, }\StringTok{"segment"}\NormalTok{, summarise, }\DataTypeTok{count =} \KeywordTok{length}\NormalTok{(segment), }
    \DataTypeTok{percentage =} \KeywordTok{round}\NormalTok{(}\KeywordTok{length}\NormalTok{(segment)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(datTrain), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       segment count percentage
## 1 Conspicuous   160       0.20
## 2       Price   200       0.25
## 3     Quality   160       0.20
## 4       Style   280       0.35
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ddply}\NormalTok{(datTest, }\StringTok{"segment"}\NormalTok{, summarise, }\DataTypeTok{count =} \KeywordTok{length}\NormalTok{(segment), }
    \DataTypeTok{percentage =} \KeywordTok{round}\NormalTok{(}\KeywordTok{length}\NormalTok{(segment)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(datTest), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       segment count percentage
## 1 Conspicuous    40       0.20
## 2       Price    50       0.25
## 3     Quality    40       0.20
## 4       Style    70       0.35
\end{verbatim}

The percentages are the same for these two sets. In practice, it is
possible that the distributions are not exactly identical but should be
close.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Divide data according to predictors
\end{enumerate}

An alternative way is to split data based on the predictors. The goal is
to get a diverse subset from a dataset so that the sample is
representative. In other words, we need an algorithm to identify the
\(n\) most diverse samples from a dataset with size \(N\). However, the
task is generally infeasible for non-trivial values of \(n\) and \(N\)
\citep{willett}. And hence practicable approaches to dissimilarity-based
selection involve approximate methods that are sub-optimal. A major
class of algorithms split the data on \emph{maximum dissimilarity
sampling}. The process starts from:

\begin{itemize}
\tightlist
\item
  Initialize a single sample as starting test set
\item
  Calculate the dissimilarity between this initial sample and each
  remaining samples in the dataset
\item
  Add the most dissimilar unallocated sample to the test set
\end{itemize}

To move forward, we need to define the dissimilarity between groups.
Each definition results in a different version of the algorithm and
hence a different subset. It is the same problem as in hierarchical
clustering where you need to define a way to measure the distance
between clusters. The possible approaches are to use minimum, maximum,
sum of all distances, the average of all distances, etc. Unfortunately,
there is not a single best choice, and you may have to try multiple
methods and check the resulted sample sets. R users can implement the
algorithm using \texttt{maxDissim()} function from \texttt{caret}
package. The \texttt{obj} argument is to set the definition of
dissimilarity. Refer to the help documentation for more details
(\texttt{?maxDissim}).

Let's use two variables (\texttt{age} and \texttt{income}) from the
customer data as an example to illustrate how it works in R and compare
maximum dissimilarity sampling with random sampling.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lattice)}
\CommentTok{# select variables}
\NormalTok{testing <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sim.dat, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Random select 5 samples as initial subset (\texttt{start}) , the rest
will be in \texttt{samplePool}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\CommentTok{# select 5 random samples}
\NormalTok{startSet <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(testing)[}\DecValTok{1}\NormalTok{], }\DecValTok{5}\NormalTok{)}
\NormalTok{start <-}\StringTok{ }\NormalTok{testing[startSet, ]}
\CommentTok{# save the rest in data frame 'samplePool'}
\NormalTok{samplePool <-}\StringTok{ }\NormalTok{testing[}\OperatorTok{-}\NormalTok{startSet, ]}
\end{Highlighting}
\end{Shaded}

Use \texttt{maxDissim()} to select another 5 samples from
\texttt{samplePool} that are as different as possible with the initical
set \texttt{start}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selectId <-}\StringTok{ }\KeywordTok{maxDissim}\NormalTok{(start, samplePool, }\DataTypeTok{obj =}\NormalTok{ minDiss, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\NormalTok{minDissSet <-}\StringTok{ }\NormalTok{samplePool[selectId, ]}
\end{Highlighting}
\end{Shaded}

The \texttt{obj\ =\ minDiss} in the above code tells R to use minimum
dissimilarity to define the distance between groups. Next, random select
5 samples from \texttt{samplePool} in data frame \texttt{RandomSet}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selectId <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(samplePool)[}\DecValTok{1}\NormalTok{], }\DecValTok{5}\NormalTok{)}
\NormalTok{RandomSet <-}\StringTok{ }\NormalTok{samplePool[selectId, ]}
\end{Highlighting}
\end{Shaded}

Plot the resulted set to compare different sampling methods:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Initial Set"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(start))}
\NormalTok{minDissSet}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Maximum Dissimilarity Sampling"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(minDissSet))}
\NormalTok{RandomSet}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Random Sampling"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(RandomSet))}
\KeywordTok{xyplot}\NormalTok{(age }\OperatorTok{~}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =} \KeywordTok{rbind}\NormalTok{(start, minDissSet, RandomSet), }\DataTypeTok{grid =} \OtherTok{TRUE}\NormalTok{, }
    \DataTypeTok{group =}\NormalTok{ group, }\DataTypeTok{auto.key =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/maxdis-1} 

}

\caption{Compare Maximum Dissimilarity Sampling with  Random Sampling}\label{fig:maxdis}
\end{figure}

The points from maximum dissimilarity sampling are far away from the
initial samples ( Fig. \ref{fig:maxdis}, while the random samples are
much closer to the initial ones. Why do we need a diverse subset?
Because we hope the test set to be representative. If all test set
samples are from respondents younger than 30, model performance on the
test set has a high risk to fail to tell you how the model will perform
on more general population.

\begin{itemize}
\tightlist
\item
  Divide data according to time
\end{itemize}

For time series data, random sampling is usually not the best way. There
is an approach to divide data according to time-series. Since time
series is beyond the scope of this book, there is not much discussion
here. For more detail of this method, see \citep{Hyndman}. We will use a
simulated first-order autoregressive model {[}AR (1){]} time-series data
with 100 observations to show how to implement using the function
\texttt{createTimeSlices\ ()} in the \texttt{caret} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulte AR(1) time series samples}
\NormalTok{timedata =}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{order=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar=}\OperatorTok{-}\NormalTok{.}\DecValTok{9}\NormalTok{), }\DataTypeTok{n=}\DecValTok{100}\NormalTok{)}
\CommentTok{# plot time series}
\KeywordTok{plot}\NormalTok{(timedata, }\DataTypeTok{main=}\NormalTok{(}\KeywordTok{expression}\NormalTok{(}\KeywordTok{AR}\NormalTok{(}\DecValTok{1}\NormalTok{)}\OperatorTok{~}\ErrorTok{~~}\NormalTok{phi}\OperatorTok{==-}\NormalTok{.}\DecValTok{9}\NormalTok{)))     }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/times-1} 

}

\caption{Divide data according to time}\label{fig:times}
\end{figure}

Fig. \ref{fig:times} shows 100 simulated time series observation. The
goal is to make sure both training and test set to cover the whole
period.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timeSlices <-}\StringTok{ }\KeywordTok{createTimeSlices}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(timedata), }
                   \DataTypeTok{initialWindow =} \DecValTok{36}\NormalTok{, }\DataTypeTok{horizon =} \DecValTok{12}\NormalTok{, }\DataTypeTok{fixedWindow =}\NormalTok{ T)}
\KeywordTok{str}\NormalTok{(timeSlices,}\DataTypeTok{max.level =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 2
##  $ train:List of 53
##  $ test :List of 53
\end{verbatim}

There are three arguments in the above \texttt{createTimeSlices()}.

\begin{itemize}
\tightlist
\item
  \texttt{initialWindow}: The initial number of consecutive values in
  each training set sample
\item
  \texttt{horizon}: the number of consecutive values in test set sample
\item
  \texttt{fixedWindow}: if FALSE, all training samples start at 1
\end{itemize}

The function returns two lists, one for the training set, the other for
the test set. Let's look at the first training sample:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get result for the 1st training set}
\NormalTok{trainSlices <-}\StringTok{ }\NormalTok{timeSlices[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{# get result for the 1st test set}
\NormalTok{testSlices <-}\StringTok{ }\NormalTok{timeSlices[[}\DecValTok{2}\NormalTok{]]}
\CommentTok{# check the index for the 1st training and test set}
\NormalTok{trainSlices[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [35] 35 36
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testSlices[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 37 38 39 40 41 42 43 44 45 46 47 48
\end{verbatim}

The first training set is consist of sample 1-36 in the dataset
(\texttt{initialWindow\ =\ 36}). Then sample 37-48 are in the first test
set ( \texttt{horizon\ =\ 12}). Type \texttt{head(trainSlices)} or
\texttt{head(testSlices)} to check the later samples. If you are not
clear about the argument \texttt{fixedWindow}, try to change the setting
to be \texttt{F} and check the change in \texttt{trainSlices} and
\texttt{testSlices}.

Understand and implement data splitting is not difficult. But there are
two things to note:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The randomness in the splitting process will lead to uncertainty in
  performance measurement.
\item
  When the dataset is small, it can be too expensive to leave out test
  set. In this situation, if collecting more data is just not possible,
  the best shot is to use leave-one-out cross-validation which is in the
  next section.
\end{enumerate}

\subsection{Resampling}\label{resampling}

You can consider resampling as repeated splitting. The basic idea is:
use part of the data to fit model and then use the rest of data to
calculate model performance. Repeat the process multiple times and
aggregate the results. The differences in resampling techniques usually
center around the ways to choose subsamples. There are two main reasons
that we may need resampling:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate tuning parameters through resampling. Some examples of models
  with such parameters are Support Vector Machine (SVM), models
  including the penalty (LASSO) and random forest.
\item
  For models without tuning parameter, such as ordinary linear
  regression and partial least square regression, the model fitting
  doesn't require resampling. But you can study the model stability
  through resampling.
\end{enumerate}

We will introduce three most common resampling techniques: k-fold
cross-validation, repeated training/test splitting, and bootstrap.

\subsubsection{k-fold cross-validation}\label{k-fold-cross-validation}

k-fold cross-validation is to partition the original sample into \(k\)
equal size subsamples (folds). Use one of the \(k\) folds to validate
the model and the rest \(k-1\) to train model. Then repeat the process
\(k\) times with each of the \(k\) folds as the test set. Aggregate the
results into a performance profile.

Denote by \(\hat{f}^{-\kappa}(X)\) the fitted function, computed with
the \(\kappa^{th}\) fold removed and \(x_i^\kappa\) the predictors for
samples in left-out fold. The process of k-fold cross-validation is as
follows:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Partition the original sample into \(k\) equal size folds
\item
  for \(\kappa=1…k\)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Use data other than fold \(\kappa\) to train the model
  \(\hat{f}^{-\kappa}(X)\)
\item
  Apply \(\hat{f}^{-\kappa}(X)\) to predict fold \(\kappa\) to get
  \(\hat{f}^{-\kappa}(x_i^\kappa)\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Aggregate the results
  \[\hat{Error} = \frac{1}{N}\Sigma_{\kappa=1}^k\Sigma_{x_i^{\kappa}}L(y_i^{\kappa},\hat{f}^{-\kappa}(x_i^\kappa))\]
\end{enumerate}
\end{quote}

It is a standard way to find the value of tuning parameter that gives
you the best performance. It is also a way to study the variability of
model performance.

The following figure represents a 5-fold cross-validation example.

\begin{figure}
\centering
\includegraphics{images/cv5fold.png}
\caption{5-fold cross-validation}
\end{figure}

A special case of k-fold cross-validation is Leave One Out Cross
Validation (LOOCV) where \(k=1\). When sample size is small, it is
desired to use as many data to train the model. Most of the functions
have default setting \(k=10\). The choice is usually 5-10 in practice,
but there is no standard rule. The more folds to use, the more samples
are used to fit model, and then the performance estimate is closer to
the theoretical performance. Meanwhile, the variance of the performance
is larger since the samples to fit model in different iterations are
more similar. However, LOOCV has high computational cost since the
number of interactions is the same as the sample size and each model fit
uses a subset that is nearly the same size of the training set. On the
other hand, when k is small (such as 2 or 3), the computation is more
efficient, but the bias will increase. When the sample size is large,
the impact of \(k\) becomes marginal.

Chapter 7 of \citep{Hastie2008} presents a more in-depth and more
detailed discussion about the bias-variance trade-off in k-fold
cross-validation.

You can implement k-fold cross-validation using \texttt{createFolds()}
in \texttt{caret}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{class<-sim.dat}\OperatorTok{$}\NormalTok{segment}
\CommentTok{# creat k-folds}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv<-}\KeywordTok{createFolds}\NormalTok{(class,}\DataTypeTok{k=}\DecValTok{10}\NormalTok{,}\DataTypeTok{returnTrain=}\NormalTok{T)}
\KeywordTok{str}\NormalTok{(cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 10
##  $ Fold01: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold02: int [1:900] 1 2 3 4 5 6 7 9 10 11 ...
##  $ Fold03: int [1:900] 1 2 3 4 5 6 7 8 10 11 ...
##  $ Fold04: int [1:900] 1 2 3 4 5 6 7 8 9 11 ...
##  $ Fold05: int [1:900] 1 3 4 6 7 8 9 10 11 12 ...
##  $ Fold06: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold07: int [1:900] 2 3 4 5 6 7 8 9 10 11 ...
##  $ Fold08: int [1:900] 1 2 3 4 5 8 9 10 11 12 ...
##  $ Fold09: int [1:900] 1 2 4 5 6 7 8 9 10 11 ...
##  $ Fold10: int [1:900] 1 2 3 5 6 7 8 9 10 11 ...
\end{verbatim}

The above code creates ten folds (\texttt{k=10}) according to the
customer segments (we set \texttt{class} to be the categorical variable
\texttt{segment}). The function returns a list of 10 with the index of
rows in training set.

\subsubsection{Repeated Training/Test
Splits}\label{repeated-trainingtest-splits}

In fact, this method is nothing but repeating the training/test set
division on the original data. Fit the model with the training set, and
evaluate the model with the test set. Unlike k-fold cross-validation,
the test set generated by this procedure may have duplicate samples. A
sample usually shows up in more than one test sets. There is no standard
rule for split ratio and number of repetitions. The most common choice
in practice is to use 75\% to 80\% of the total sample for training. The
remaining samples are for validation. The more sample in the training
set, the less biased the model performance estimate is. Increasing the
repetitions can reduce the uncertainty in the performance estimates. Of
course, it is at the cost of computational time when the model is
complex. The number of repetitions is also related to the sample size of
the test set. If the size is small, the performance estimate is more
volatile. In this case, the number of repetitions needs to be higher to
deal with the uncertainty of the evaluation results.

We can use the same function (\texttt{createDataPartition\ ()}) as
before. If you look back, you will see \texttt{times\ =\ 1}. The only
thing to change is to set it to the number of repetitions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{segment, }\DataTypeTok{p =}\NormalTok{ .}\DecValTok{8}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{times =} \DecValTok{5}\NormalTok{)}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(trainIndex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  int [1:800, 1:5] 1 3 4 5 6 7 8 9 10 11 ...
##  - attr(*, "dimnames")=List of 2
##   ..$ : NULL
##   ..$ : chr [1:5] "Resample1" "Resample2" "Resample3" "Resample4" ...
\end{verbatim}

Once know how to split the data, the repetition comes naturally.

\subsubsection{Bootstrap Methods}\label{bootstrap-methods}

Bootstrap is a powerful statistical tool (a little magic too). It can be
used to analyze the uncertainty of parameter estimates
\citep{bootstrap1986} quantitatively. For example, estimate the standard
deviation of linear regression coefficients. The power of this method is
that the concept is so simple that it can be easily applied to any model
as long as the computation allows. However, you can hardly obtain the
standard deviation for some models by using the traditional statistical
inference.

Since it is with replacement, a sample can be selected multiple times,
and the bootstrap sample size is the same as the original data. So for
every bootstrap set, there are some left-out samples, which is also
called ``out-of-bag samples.'' The out-of-bag sample is used to evaluate
the model. Efron points out that under normal circumstances
\citep{efron1983}, bootstrap estimates the error rate of the model with
more certainty.The probability of an observation \(i\) in bootstrap
sample B is:

\(\begin{array}{ccc} Pr{i\in B} & = & 1-\left(1-\frac{1}{N}\right)^{N}\\  & \approx & 1-e^{-1}\\  & = & 0.632 \end{array}\)

On average, 63.2\% of the observations appeared at least once in a
bootstrap sample, so the estimation bias is similar to 2-fold
cross-validation. As mentioned earlier, the smaller the number of folds,
the larger the bias. Increasing the sample size will ease the problem.
In general, bootstrap has larger bias and smaller uncertainty than
cross-validation. Efron came up the following ``.632 estimator'' to
alleviate this bias:

\[(0.632 × original\ bootstrap\ estimate) + (0.368 × apparent\ error\ rate)\]

The apparent error rate is the error rate when the data is used twice,
both to fit the model and to check its accuracy and it is apparently
over-optimistic. The modified bootstrap estimate reduces the bias but
can be unstable with small samples size. This estimate can also be
unduly optimistic when the model severely over-fits since the apparent
error rate will be close to zero. Efron and Tibshirani \citep{b632plus}
discuss another technique, called the ``632+ method,'' for adjusting the
bootstrap estimates.

\chapter{Measuring Performance}\label{measuring-performance}

\chapter{Tree-Based Methods}\label{tree-based-methods}

The tree-based models can be used for regression and classification.
They are popular tools for many reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Do not require user to specify the form of the relationship between
  predictors and response
\item
  Do not require (or if they do, very limited) data preprocessing and
  can handle different types of predictors (sparse, skewed, continuous,
  categorical, etc.)
\item
  Robust to co-linearity
\item
  Can handle missing data
\item
  Many pre-built packages make implementation as easy as a button push
\end{enumerate}

\begin{figure}
\centering
\includegraphics{../linhui.org/book/Figure/treeEN.png}
\caption{}
\end{figure}

\section{Splitting Criteria}\label{splitting-criteria}

\section{Tree Pruning}\label{tree-pruning}

\section{Regression and Decision Tree
Basic}\label{regression-and-decision-tree-basic}

\section{Bagging Tree}\label{bagging-tree-1}

\section{Random Forest}\label{random-forest}

\section{Gradient Boosted Machine}\label{gradient-boosted-machine}

\bibliography{bibliography.bib}

\backmatter
\printindex

\end{document}
